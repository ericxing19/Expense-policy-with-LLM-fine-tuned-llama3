{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 27 10:23:08 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:25:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             66W /  300W |   17215MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000000:5B:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             65W /  300W |   26155MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off |   00000000:9B:00.0 Off |                    0 |\n",
      "| N/A   42C    P0             82W /  300W |   68703MiB /  81920MiB |     38%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off |   00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   68C    P0            183W /  300W |   39143MiB /  81920MiB |     56%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4182      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    0   N/A  N/A    574390      C   python                                       2062MiB |\n",
      "|    0   N/A  N/A    911277      C   python                                       2060MiB |\n",
      "|    0   N/A  N/A   1980452      C   /home/uceedw1/oct/bin/python                13056MiB |\n",
      "|    1   N/A  N/A      4182      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    1   N/A  N/A    586368      C   /home/uceedw1/oct/bin/python                11042MiB |\n",
      "|    1   N/A  N/A   4090571      C   /home/uceedw1/oct/bin/python                15084MiB |\n",
      "|    2   N/A  N/A      4182      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    2   N/A  N/A    495266      C   python3                                     68676MiB |\n",
      "|    3   N/A  N/A      4182      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    3   N/A  N/A    585737      C   /usr/bin/python3.11                         21572MiB |\n",
      "|    3   N/A  N/A    616203      C   /usr/bin/python3.11                         17540MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseudocode for current project baseline:\n",
    "Initialise policy\n",
    "Initialise Test_set, Training_set\n",
    "Initialise training_argments\n",
    "# prepare data for orpo fine-tune\n",
    "For each sample in Training_set do:\n",
    "    sample.Format_data(policy)\n",
    "end For\n",
    "\n",
    "Run orpo_fine-tuning with base_model, training_argments\n",
    "Until epoch >= training_argments.num_of_epoch or validation_loss_difference <= training_argments.threshold\n",
    "\n",
    "# evaluation both base model and fine-tuned model\n",
    "For model in [base_model, fine-tuned_model] do:\n",
    "    For prompt in Test_set do:\n",
    "        if use_RAG:\n",
    "            rag_result <- search(prompt, Training_set)\n",
    "            response <- model.inference_with_RAG(policy, prompt, rag_result)\n",
    "        else:\n",
    "            response <- model.inference(policy, prompt)\n",
    "    end For\n",
    "    response.evluate()\n",
    "end For"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseudocode for proposed GAN project baseline:\n",
    "Initialise policy\n",
    "Initialise Test_set, Training_set\n",
    "Initialise training_argments\n",
    "# prepare data for orpo fine-tune\n",
    "For each sample in Training_set do:\n",
    "    sample.Format_data(policy)\n",
    "end For\n",
    "\n",
    "For epoch in num_of_epoch do:\n",
    "    For prompt in Training_set do:\n",
    "        response <- model.inference(policy, prompt)\n",
    "        discriminator.train(Training_set.GPT_response, response)\n",
    "        score = discriminator.inference(response)\n",
    "\n",
    "        generation_loss = compute_generation_loss(generated_response, target_response)\n",
    "        # or SFT loss if possible\n",
    "        \n",
    "        scoring_loss = torch.nn.functional.mse_loss(generated_score, target_score)\n",
    "        \n",
    "        total_loss = generation_loss + lambda * scoring_loss\n",
    "\n",
    "        discriminator.update_and_backprop()\n",
    "        model.update_and_backprop()\n",
    "\n",
    "    end For\n",
    "end For\n",
    "\n",
    "# evaluation both base model and fine-tuned model\n",
    "For model in [base_model, fine-tuned_model] do:\n",
    "    For prompt in Test_set do:\n",
    "        if use_RAG:\n",
    "            rag_result <- search(prompt, Training_set)\n",
    "            response <- model.inference_with_RAG(policy, prompt, rag_result)\n",
    "        else:\n",
    "            response <- model.inference(policy, prompt)\n",
    "    end For\n",
    "    response.evluate()\n",
    "end For"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/uceehuf/.local/lib/python3.11/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/uceehuf/.local/lib/python3.11/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/lib64/python3.11/site-packages (from rouge_score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/uceehuf/.local/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/uceehuf/.local/lib/python3.11/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/uceehuf/.local/lib/python3.11/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/uceehuf/.local/lib/python3.11/site-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/uceehuf/.local/lib/python3.11/site-packages (from nltk->rouge_score) (4.66.2)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=7383674a7f7aec344faa64897f371cc50b5a7ad505dd488c7a79cd24da42ffef\n",
      "  Stored in directory: /home/uceehuf/.cache/pip/wheels/6f/d0/79/3beb1118c3c80946f4d13c0200a43cf56f35fc76ce6aac08ac\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "reference = [['this', 'is', 'a', 'test']]\n",
    "candidate = ['this', 'is','a', 'test']\n",
    "\n",
    "# Calculate BLEU score\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "print(f\"BLEU score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.6667\n",
      "ROUGE-L: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = \"This is a test.\"\n",
    "candidate = \" is test.\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference, candidate)\n",
    "print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "print(f\"ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "text = \"This is a test sentence.\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "print(f\"Perplexity: {perplexity.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
