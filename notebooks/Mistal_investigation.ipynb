{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceehuf/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-18 10:10:15.872904: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-18 10:10:15.913024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-18 10:10:15.913055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-18 10:10:15.914341: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-18 10:10:15.921563: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 10:10:18.110136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 23 13:29:58 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:25:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             69W /  300W |    6223MiB /  81920MiB |      6%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000000:5B:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             39W /  300W |      17MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off |   00000000:9B:00.0 Off |                    0 |\n",
      "| N/A   69C    P0            301W /  300W |   46085MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off |   00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   63C    P0            210W /  300W |   11731MiB /  81920MiB |     98%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4182      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    0   N/A  N/A    574390      C   python                                       2062MiB |\n",
      "|    0   N/A  N/A    911277      C   python                                       2060MiB |\n",
      "|    0   N/A  N/A   2647654      C   python                                       2062MiB |\n",
      "|    1   N/A  N/A      4182      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    2   N/A  N/A      4182      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    2   N/A  N/A   2472759      C   python                                      11194MiB |\n",
      "|    2   N/A  N/A   2645205      C   /usr/bin/python3.11                         34734MiB |\n",
      "|    3   N/A  N/A      4182      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    3   N/A  N/A   2647207      C   python                                      11582MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if the GPU is available\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a fortnight, I relied on a TFL travel card for my daily commute from home to the GNEI campus, totaling £50 in expenses. What's the reimbursement amount?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = '../datasets/Train_Data.xlsx'\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# Load the Excel file uing pandas\n",
    "data_frame = pd.read_excel(file_path,sheet_name=1, engine='openpyxl')\n",
    "\n",
    "np_frame = np.array(data_frame['Prompt'])\n",
    "print(data_frame['Prompt'][0])\n",
    "\n",
    "def generate_rejected(data_frame):\n",
    "    \"\"\"\n",
    "    pre-process the dataset to generate rejected answer from other prompts\n",
    "    \"\"\"\n",
    "    random_seed = 202465\n",
    "    random.seed(random_seed)\n",
    "    rejected_T1 = []\n",
    "    rejected_Amount = []\n",
    "    rejected_T2 = []\n",
    "    # rejected_reason = []\n",
    "\n",
    "    T2_labels = ['Fully Reimbursable','Partially Reimbursable','Not Reimbursable','Further Clarification Required']\n",
    "\n",
    "    for i in range(0,len(data_frame)):\n",
    "        all_change_made = False\n",
    "        while not all_change_made:\n",
    "            # T1 toggle:\n",
    "            if data_frame['Classification T1'][i] == 'Policy Not Violated':\n",
    "                T1 = 'Policy Violated'\n",
    "            else:\n",
    "                T1 = 'Policy Not Violated'\n",
    "            # Amount ramdom number:\n",
    "            Am = round(random.uniform(0, 1000),2)\n",
    "\n",
    "            # T2 randomly choose a new label from the remaining labels\n",
    "            remaining_labels = [label for label in T2_labels if label != data_frame['Classification T2'][i]]\n",
    "    \n",
    "            T2 = random.choice(remaining_labels) \n",
    "\n",
    "            if (T1 != data_frame['Classification T1'][i]) and (Am != data_frame['Amount'][i]) and (T2 != data_frame['Classification T2'][i]):\n",
    "                all_change_made = True\n",
    "\n",
    "\n",
    "        # Following was codes for only 1 field changed\n",
    "        # change_made = False\n",
    "        # counter = 0\n",
    "        # while not change_made:\n",
    "            \n",
    "        #     binary_change = bin(random.randint(1, 15))[2:].zfill(4)\n",
    "        #     counter += 1\n",
    "        #     rand_index = random.randint(0, len(data_frame)-1)\n",
    "        #     if binary_change[0] == '1':\n",
    "        #         T1 = data_frame['Classification T1'][rand_index]\n",
    "        #     else:\n",
    "        #         T1 = data_frame['Classification T1'][i]\n",
    "        #     if binary_change[1] == '1':            \n",
    "        #         Am = data_frame['Amount'][rand_index]\n",
    "        #     else:\n",
    "        #         Am = data_frame['Amount'][i]\n",
    "        #     if binary_change[2] == '1':\n",
    "        #         T2 = data_frame['Classification T2'][rand_index]\n",
    "        #     else:\n",
    "        #         T2 = data_frame['Classification T2'][i]\n",
    "        #     # if binary_change[3] == '1':\n",
    "        #     #     Re = data_frame['Reasons'][rand_index]...\n",
    "\n",
    "        #     if (T1 != data_frame['Classification T1'][i]) or (Am != data_frame['Amount'][i]) or (T2 != data_frame['Classification T2'][i]):\n",
    "        #         change_made = True\n",
    "        #     elif counter > 10:\n",
    "        #         print(f\"generate failed for prompt {i}\")\n",
    "\n",
    "        # now that changed are made:\n",
    "        rejected_T1.append(T1)\n",
    "        rejected_Amount.append(Am)\n",
    "        rejected_T2.append(T2)\n",
    "        # rejected_Re.append(Re)\n",
    "\n",
    "    rejected_df = pd.DataFrame({\n",
    "        'rejected_T1': rejected_T1,\n",
    "        'rejected_Amount': rejected_Amount,\n",
    "        'rejected_T2': rejected_T2,\n",
    "        # 'rejected_reasons': rejected_Re,\n",
    "    })\n",
    "\n",
    "    # Concatenate the original DataFrame with the rejected DataFrame\n",
    "    data_combine = pd.concat([data_frame, rejected_df], axis=1)\n",
    "    return data_combine\n",
    "\n",
    "temp1 = generate_rejected(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Prompt', 'Category', 'Classification T1', 'Classification T2', 'Amount', 'Test ID', 'rejected_T1', 'rejected_Amount', 'rejected_T2', '__index_level_0__'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "temp = temp1[temp1[\"Category\"] == \"Accomodation and Sustenance\"]\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "tr_data = Dataset.from_pandas(temp)\n",
    "\n",
    "print(tr_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "# Load tokenizer and model\n",
    "# Mistral 7B it\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    trust_remote_code=True, device_map=\"auto\", torch_dtype=torch_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44769\n",
      "11553\n"
     ]
    }
   ],
   "source": [
    "context_full = \"\"\"\n",
    "\n",
    "GNEI Expenses Policy\n",
    "Summary Guidance\n",
    "Subject\n",
    "Page No.\n",
    "Train Travel\n",
    "This should be standard class unless a heavily discounted first-class ticket is booked significantly in advance of the date of travel via GNEI's approved travel provider.\n",
    "\n",
    "\n",
    "Taxis\n",
    "Taxis may be used where there is a clear requirement or where they are the most economic practical means of transport.\n",
    "\n",
    "\n",
    "Business Mileage\n",
    "Cars - 50p per mile for the first 11,000 miles in a tax year and 28p per mile thereafter, Motorcycles - 26p per mile, Bicycles - 22p per mile.\n",
    "\n",
    "\n",
    "Air Travel\n",
    "This should be booked using GNEI’s preferred travel provider where possible. For flights of five hours or less, staff should book restricted tickets in economy. For flights lasting five hours or longer staff may travel premium economy. Where a long international flight is immediately followed by a presentation or meeting, staff may travel by business class.\n",
    "\n",
    "\n",
    "Hotel Rates\n",
    "UK: should not exceed £210 including VAT (room only) in London or £140 including VAT (room only) outside of London. The cost of accommodation at or near the normal place of work wouldn’t qualify as business travel and would therefore be subject to tax.\n",
    "Overseas: not subject to a fixed rate or limit. Claimants should instead look to apply this policy's general principles (section 3) when assessing reasonableness and use either HMRC's overseas benchmark rates and / or this policy's equivalent UK rates for guidance\n",
    "\n",
    "\n",
    "Meals whilst away overnight\n",
    "UK Breakfast: maximum of £12 receipted including VAT and service; unreceipted £5.5\n",
    "UK Lunch: maximum of £18 receipted including VAT and service; unreceipted £6\n",
    "UK Dinner: maximum of £36 receipted including VAT and service, unreceipted £17.\n",
    "UK 24-hour rate: In line with the above, up to £66 receipted (£28 unreceipted).\n",
    "Overseas: not subject to a fixed rate or limit. Claimants should instead look to apply this policy's general principles (section 3) when assessing reasonableness and use either HMRC's overseas benchmark rates and / or this policy's equivalent UK rates for guidance.\n",
    "\n",
    "\n",
    "Line Rental\n",
    "Line rental cannot be claimed; only the cost of business calls on a call-by-call basis can be claimed.\n",
    "\n",
    "\n",
    "Broadband\n",
    "Home internet connection cannot be claimed.\n",
    "\n",
    "\n",
    "Staff and Student Entertaining\n",
    "This means food or drink for two or more members of staff or registered GNEI students in connection with GNEI business activities. The cost of entertaining should not exceed £22 per head.\n",
    "\n",
    "\n",
    "Staff Social Functions\n",
    "Events that are not in connection to GNEI business activities e.g. Christmas lunches/parties, end of term socials, retirement parties etc. The GNEI department contribution should be no more than £22 per head.\n",
    "\n",
    "\n",
    "Business entertaining\n",
    "Cost should be appropriate and not exceed £44 per head including alcohol and service unless exceptional circumstances apply.\n",
    "\n",
    "\n",
    "Professional Subscriptions\n",
    "GNEI will only reimburse staff, or pay on their behalf, annual subscriptions or memberships to a professional body where either:\n",
    "Savings to GNEI arising from membership, for example reduced conference attendance fees, exceed the cost of membership, or\n",
    "Membership is mandatory in order to be able to teach on a professionally accredited course.\n",
    "In all cases, the professional body must feature in the list of approved organisations published by HMRC.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Introduction \n",
    "This Expenses Policy (‘the Policy’) provides guidance to all individuals claiming reimbursement of reasonable expenses incurred in connection with GNEI business.\n",
    "GNEI is a charity and a large recipient of public funds from UKRI and the Office for Students, grants from other public bodies, charities and fees paid by students. All expenditure should be appropriate and modest in scale.\n",
    "This policy has been prepared in accordance with Income Tax and National Insurance Contribution regulations and Her Majesty’s Revenue and Customs (HMRC) regulations.\n",
    "\n",
    "\n",
    "\n",
    "2. Scope\n",
    "This policy applies to all spend on GNEI activities including from research grants and discretionary accounts as well as departmental codes. \n",
    "This policy covers the following areas of expenditure:\n",
    "Travel\n",
    "Overnight costs and allowances\n",
    "Telephone and internet costs\n",
    "Entertainment & Hospitality\n",
    "Training\n",
    "Subscriptions\n",
    "Advances\n",
    "Other Expenditure\n",
    "For items that do not fall under any of the above headings, please refer to your local finance team or Accounts Payable at expenses@gnei.ac.uk.\n",
    "For expense claim procedures, please refer to Annex 1.\n",
    "Generally, GNEI will not reimburse individuals for:\n",
    "Broadband/Internet Provision\n",
    "Equipment\n",
    "Fines\n",
    "Gifts\n",
    "Insurance\n",
    "Mobile Phone Contracts/Hardware\n",
    "Personal Expenditure\n",
    "Professional Subscriptions\n",
    "Stationery\n",
    "For further details on the above exclusions, please refer to Annex 2.\n",
    "Information on VAT aspects which are common to all claims can be found at Annex 3.\n",
    "This policy also provides information on subsistence payments for visiting Research Fellows, please refer to Annex 4.\n",
    "Childcare and caring responsibility can be claimed in certain circumstances, please refer to Annex 5.\n",
    "\n",
    "\n",
    "\n",
    "3. General Principles\n",
    "GNEI employees and all others engaged in GNEI activity (collectively claimants) will be reimbursed for the actual cost of expenses incurred, wholly, exclusively and necessarily in the performance of GNEI activity as prescribed in this policy.\n",
    "The expense must be justifiable and reasonable according to the information in this policy, and the claim should always be prepared honestly, legally and responsibly. Any breach of the policy could lead to disciplinary action, up to and including dismissal.\n",
    "Whilst this policy aims to provide comprehensive guidance on reimbursable out-of- pocket expenditure, it is recommended that staff still seek advance approval from their budget holder or department manager in any situation where interpretation of this policy is in doubt.\n",
    "Employees should be able to procure the majority of goods and services required for GNEI business using the standard purchase to payment procedures using the GNEIFinance system.\n",
    "It is the responsibility of authorised signatories to ensure that this expenses policy is upheld; and it should be noted that individual claims falling outside of the policy may be subject to review and/or rejection by Finance.\n",
    "Electronic images of original receipts must accompany all claims. Credit card slips or credit card/bank statements will not be accepted as evidence of business expenditure. All receipts used should include details of what goods or services have been purchased. Original paper receipts do not need to be stored locally, unless they relate to sponsored research and it is explicit in the terms of the grant that original paper receipts need to be retained.\n",
    "When making a claim for a group, the most senior person present should pay for the expenditure and make the claim.\n",
    "Expenses should be submitted as soon as possible after they have been incurred. Claims must be made within three months. Approval should be gained from the relevant School or PS Head of Finance if the claim is over three months old.\n",
    "It costs a fixed amount to process any claim, so claimants should endeavour not to submit claims for less than £33, unless they represent the total of expenses in a three-month period.\n",
    "If a claimant inadvertently makes an error with an expense claim and has been reimbursed, GNEI will recover the amount from them.\n",
    "Expenses must not be used as a way of rewarding people or encouraging them to work in remote locations.\n",
    "GNEI will not meet the cost of expenses for spouses, partners, other family members or friends of GNEI staff.\n",
    "Claims are checked by Accounts Payable staff and any deemed to be fraudulent will be investigated and referred to GNEI’s Internal Auditors. If a claimant is found to have submitted a fraudulent claim they will face disciplinary proceedings, leading to penalties up to and including summary dismissal.\n",
    "\n",
    "\n",
    "\n",
    "4. Travel\n",
    "This section details what can be claimed as travel expenses and provides information on specific modes of transport and other expenses associated with travel.\n",
    "\n",
    "General guidance for travelling on GNEI business\n",
    "The cost of business travel, that is, journeys away from your normal place of work while undertaking GNEI business can be claimed.\n",
    "In line with the GNEI travel policy, all business trips in the UK or abroad must be authorised in writing in advance by the GNEI budget holder responsible for making the funds available. It is recommended that this is attached to the claim.\n",
    "The budget holder / manager must agree that the travel is necessary and that alternative methods such as conference call, video conference, phone or email cannot be used or are not appropriate. Consideration should also be given as to how to minimise the environmental impact of travel.\n",
    "This authorisation ensures that anticipated costs are in line with departmental plans, are eligible under the terms of any associated funding, and that subsequent expense claims can be submitted with confidence, subject to the provisions of this policy.\n",
    "GNEI has preferred travel suppliers for airline, hotel and car hire bookings details on these suppliers can be found on the Procurement website. These suppliers should be used for travel bookings where possible.\n",
    "\n",
    "Private travel costs\n",
    "The following cannot be claimed:\n",
    "Travel between home and normal place of work.\n",
    "Business travel broadly similar to the claimant’s normal commute.\n",
    "Recreational travel and accommodation at or near the business travel destination.\n",
    "Travel or accommodation for family or friends accompanying the claimant on the business journey.\n",
    "\n",
    "Rate of exchange\n",
    "If a credit card was used to pay for overseas expenses, the rate charged by the credit card issuer can be claimed. A copy of the statement should be used to evidence the rate used.\n",
    "For employees GNEIExpenses will use a default rate for translating foreign currency transactions; this can be overridden if different to the rate at the time of expenditure and fluctuations will be allowed within a set tolerance. If the automatic rate is overridden, a copy of the bank statement should be included with the claim to back up the rate used.\n",
    "Buying foreign currency is not a genuine business expense and cannot be claimed.\n",
    "\n",
    "Travel Insurance\n",
    "Travel Insurance needs to be activated before traveling overseas, travelling in the UK for fieldtrips or travelling on a business trip where there is an overnight stay or air travel involved.\n",
    "Insurance can be activated by completing a travel insurance request form, which can be found in the Insurance Section on the Finance website.\n",
    "The Insurance team will issue a cover note and provide details of the policy. There is no cost to departments for travel insurance. For regular travellers the Insurance Section can issue annual cover notes on request.\n",
    "If a claimant has an existing medical condition and is not travelling against their doctors’ orders, this is covered by the GNEI Travel Insurance Policy at no extra cost.\n",
    "Personal travel insurance cannot be claimed as GNEI already provides cover for business travel, please refer to Annex 2 for further information on this exclusion.\n",
    "\n",
    "Public Transport\n",
    "Journeys made on rail, bus, ferry, river boat, underground, metro or tram services – can be claimed.\n",
    "The most economical method of travel should be used. For example, if the claimant uses public transport in London regularly, they should use an Oyster card rather than paying for single journeys by cash.\n",
    "\n",
    "Train journeys\n",
    "These should be standard class unless a heavily discounted first-class ticket is booked significantly in advance of the date of travel.\n",
    "If the claimant does travel first class, an explanation must be included on the claim and if necessary the claim will be passed to the relevant School or PS Head of Finance for approval.\n",
    "Trains should be booked in advance where possible via GNEI's approved travel provider or similar rail travel provider as this is the most cost effective option. Tickets can then be collected at the relevant station or sent via post.\n",
    "\n",
    "Tube and bus journeys in London\n",
    "If a London Transport Oyster or contactless credit/debit card is used for business journeys, the details of each individual journey should be entered on the claim, being specific about locations, date and times and purpose of journey.\n",
    "Only the relevant journey should be claimed, not the whole top-up cost, unless the whole top-up will be used for GNEI business travel. Receipts are not necessary for individual journeys but are required if claiming for the whole top- up fee.\n",
    "\n",
    "Receipts\n",
    "Relevant tickets or documentation should be kept and attached to the expense claim electronically.\n",
    "Claimants who have a rail or Oyster season ticket enabled to use the underground can only claim for travel if extra expenses are incurred that are not covered by the season ticket and can provide a receipt as proof of the expense.\n",
    "Ticket machines will provide receipts if the ticket is retained by the automatic barrier at the end of a journey.\n",
    "\n",
    "Taxis\n",
    "Taxi fares can only be claimed for journeys where:\n",
    "It is clearly the most efficient and cost-effective method of transport.\n",
    "Alternative methods of transport are impractical due to pregnancy, disability, illness or injury, luggage or similar.\n",
    "A member of staff is working in the office very late, i.e. after 11pm.\n",
    "Prior approval has been given, where feasible, from the budget holder or departmental administrator/manager.\n",
    "\n",
    "Business Mileage\n",
    "If a car, motorcycle or bicycle has been used on a business trip the costs can be claimed using a mileage allowance – that is, a sum of money for every business mile travelled. GNEI does not reimburse claims based on actual petrol receipts.\n",
    "GNEI has standard rates for these allowances, depending on the circumstances surrounding the trip (see below).\n",
    "Only additional costs incurred, above what would normally be spent for commuting to work (which are not eligible as a business expense) can be claimed.\n",
    "A car should only be used if it is cheaper and more efficient than public transport. Cars should be shared with other staff members wherever possible.\n",
    "The claim should state the start and end points of the journey, along with the number of miles being claimed.\n",
    "Mileage is paid at the HMRC approved mileage allowance rates:\n",
    "Cars - 50p per mile for the first 11,000 miles in a tax year and 28p per mile thereafter.\n",
    "Motorcycles - 26p per mile.\n",
    "Bicycles - 22p per mile.\n",
    "\n",
    "Car/bicycle/motorcycle insurance\n",
    "It is important that adequate personal insurance cover for business use is in place, as cars, bicycles or motorcycles belonging to claimants are not covered by GNEI’s insurance. Personal insurance cannot be claimed, see Annex 2 for further information.\n",
    "If a staff member uses or allows another member of staff to use a vehicle on GNEI’s business that is insured for third party risks only, GNEI will not be held liable for any of the following:\n",
    "damage or repairs to the vehicle\n",
    "property lost or damaged\n",
    "personal injuries sustained or caused during or as a result of such use.\n",
    "\n",
    "Car sharing\n",
    "Only the driver can claim mileage for a journey where two or more GNEI staff share a car. The following cannot be claimed:\n",
    "Insurance cover for private cars.\n",
    "Accident or breakdown recovery costs, such as AA membership.\n",
    "\n",
    "Air Travel\n",
    "Air travel should be booked using GNEI’s preferred travel provider where possible.\n",
    "GNEI prefers lower fares, based on flight arrival and departure times which cannot be changed. Where there is a need for flexibility in departure or arrival times, a different ticket option will have to be requested. Flexible tickets for flights under three hours require approval by the relevant Dean / PS Director, Faculty Director of Operations or School/PS Head of Finance.\n",
    "Membership of a frequent traveller scheme and potential air miles benefits must not deflect from using the most cost-effective airline option for travel.\n",
    "GNEI does not meet the travelling costs of:\n",
    "Employees of contractors\n",
    "Spouses, partners, other family members or friends.\n",
    "Classes of travel:\n",
    "For flights of five hours or less, staff should book restricted tickets in economy.\n",
    "For flights lasting five hours or longer staff may travel premium economy.\n",
    "Where a long international flight is immediately followed by a presentation or meeting, staff may travel by business class. This must be pre-approved by the relevant Dean / PS Director, Faculty Director of Operations or School/PS Head of Finance.\n",
    "\n",
    "Toll charges\n",
    "Toll charges paid for in the course of a business journey can be claimed. Toll bridges or roads rarely give receipts; therefore the name of the toll bridge or road should be included on the expense claim.\n",
    "\n",
    "Congestion charges\n",
    "GNEI will not reimburse the cost of any congestion charge for staff or visitors except in exceptional circumstances, e.g. where a member of staff has to collect/deliver equipment to a location and the journey could not be completed on public transport. This must be approved in advance by the relevant School/PS Head of Finance.\n",
    "\n",
    "Car Parking\n",
    "Reasonable parking costs on business visits and journeys away from the office can be claimed.\n",
    "Hotel parking costs can be claimed if they are charged separately on the bill.\n",
    "If a claimant has to drive to their place of work as part of a business journey, e.g. to collect equipment, parking costs can be claimed. Details of the reason should be included with the claim.\n",
    "The cost of parking at an entertainment venue cannot be claimed.\n",
    "VAT can be reclaimed (where it's charged) on parking fees which cost less than £28 even if the claimant is not able to get a VAT receipt.\n",
    "\n",
    "Car Hire\n",
    "Approval must be obtained from the budget holder or departmental administrator/manager before a car is hired.\n",
    "If hiring a car is the cheapest mode of travel, it should be booked through GNEI's approved car hire provider (for UK or overseas).\n",
    "The car should only be used for business purposes.\n",
    "Rates are inclusive of CDW (Collision Damage Waiver), TPI (Third Party Insurance) and PAI (Personal Accident Insurance).\n",
    "Any petrol bought solely for business purposes can be claimed. A VAT receipt should be obtained for purchases, a credit card receipt is not sufficient.\n",
    "\n",
    "Group Travel\n",
    "This is in relation to a staff conference or student field trip. Prior approval from the budget holder or departmental administrator/manager should be obtained before booking a group trip.\n",
    "The same rules apply for each category of travel and overnight costs as for travelling as an individual.\n",
    "All travel (including coach hire) and overnight costs should be booked in advance.\n",
    "GNEI's approved travel provider provides a group travel & conference booking facility, where all elements of the trip can be arranged. This can be used for arranging a business conference, as well as student field trips.\n",
    "Please ensure adequate insurance is in place for all staff and students on the trip. Travel insurance needs to be activated prior to the trip by using the Insurance section of the Finance website.\n",
    "\n",
    "Travel Incidentals\n",
    "Claimants can claim the following goods or services if they are related to the business purpose of the trip:\n",
    "The cost of obtaining a visa for working abroad on GNEI business. These can be obtained from GNEI's approved travel provider.\n",
    "The cost of any vaccinations needed for working abroad.\n",
    "There may be occasions when a claimant needs a second passport, for example when travel to one country could lead to immigration difficulties in travelling on business to some other countries. GNEI will pay this expense.\n",
    "Approval must be obtained in advance from the budget holder or departmental administrator/manager.\n",
    "\n",
    "\n",
    "\n",
    "5. Overnight costs and allowances\n",
    "Such costs are subject to the requirement for pre-approval in section 4.3 above.\n",
    "\n",
    "Hotels\n",
    "In line with the GNEI travel policy, Hotels must be booked via GNEI's approved travel provider for both overseas and UK travel, with recovery through expenses only permitted in those cases set out in Section 3.1 of that policy and subject to an appropriate level of advance approval as set out in section 3.2. The approval obtained must be attached to submitted expense claims alongside receipts for the room cost, breakfast, dinner and other eligible incidentals.\n",
    "Hotel city tax: from time to time, additional local city tax charges may apply for accommodation. These can be reclaimed via expenses.\n",
    "\n",
    "UK Hotels\n",
    "As stipulated by HMRC, the cost of accommodation at or near the normal place of work does not qualify as business travel; any claim would be subject to tax on the individual as a reimbursement of personal expenditure. Such costs may not be claimed, and staff are asked to bear this in mind when making their travel arrangements.\n",
    "The cost of a hotel in the UK should not exceed £210 including VAT (room only) in London or £140 including VAT (room only) outside of London. If a hotel cost is higher than these limits, this should be pre-approved by the relevant Dean / PS Director, Faculty Director of Operations or School/PS Head of Finance.\n",
    "\n",
    "Overseas hotels\n",
    "The cost of overseas accommodation may vary significantly from country to country. Both claimants and approvers should look to the general principles of this policy therefore when incurring such costs and when determining an appropriate level of claim: in all respects expenses must be justifiable, reasonable and represent value for money.\n",
    "With this in mind, the University does not look to prescribe fixed limits for overseas accommodation but instead provide guidance on what a reasonable claim would be so that claimants and approvers are able to make a fully informed assessment.\n",
    "In the first instance, employees should expect to be reimbursed for hotels that are equivalent to the most appropriate GBP rate for UK travel detailed in section 5.5. For overseas stays the expectation of the University is that an employee should book accommodation that is equivalent to a UK 3-star level. For areas where there is deprivation or a high security risk then the rating may be increased to 4-star.\n",
    "Employees may also consult HMRC's scale rates for overseas travel in order to judge what is reasonable when quoted in local currency. These rates do not represent an upper limit nor an automatic entitlement but should be considered a point of reference only.\n",
    "Employees should be aware however that claims materially in excess of the HMRC rates are likely to be subject to challenge either during the approval process or before settlement by the central processing teams. Where this is expected to be the case, claimants should ensure that clear justification and confirmation of pre-approval from the relevant budget holder is attached to the claim as per Section 4.3.\n",
    "Where attendance at an international conference requires specific accommodation (for example by virtue of a fixed attendance package or as a necessary means of realising the benefits of the conference), this factor should also be documented with pre-approval attached to the claim.\n",
    "Incidentals\n",
    "All necessary incidentals (including brief personal calls home) should be receipted and claimed:\n",
    "Newspapers, bar drinks, mini bar, hotel video and health and fitness facilities cannot be claimed.\n",
    "Laundry costs can only be claimed if absolutely necessary, the cost is reasonable and appropriate and the stay away is for at least 5 consecutive nights.\n",
    "\n",
    "Telephone, Fax and Internet Charges\n",
    "The cost of business calls, fax and internet access charges for business use, as long as they are included on an itemised bill, can be claimed.\n",
    "When working overseas a telephone card can be purchased as this reduces the cost of local calls, the cost of this instead of calls from hotels can be claimed.\n",
    "\n",
    "Meals\n",
    "When travelling on GNEI business, employees are entitled to recover the cost of reasonable subsistence, namely breakfast, lunch, and dinner. Meals cannot be claimed if already included in hotel accommodation rates being claimed for.\n",
    "If several members of staff have a meal together, the most senior person should pay and submit the expense claim. In such a case no-one other than the most senior person should claim an allowance for the meals. Please ensure that it is made clear in the justification field of GNEIExpenses that the receipt is covering more than one employee’s meal, and each employee name (covered by the receipt) should be listed.\n",
    "The following rates include the cost of alcoholic beverages.\n",
    "\n",
    "UK Meals\n",
    "Breakfast. If breakfast is not included in the hotel room rate, the cost can be claimed, which should be receipted and claimed as part of the overnight stay to a maximum of £12 including VAT and service. If it is not possible to obtain a receipt, then £5.5 may be claimed for breakfast.\n",
    "Lunch. If the claimant is working away from their normal place of work, lunch can be claimed to a maximum of £18 including VAT and service. If it is not possible to obtain a receipt, then £5.5 may be claimed for lunch.\n",
    "Evening meal. The cost of an evening meal can be claimed which should be receipted and claimed as part of the overnight stay to a maximum of £36 including VAT and service. If it is not possible to obtain a receipt, then £17 may be claimed for dinner.\n",
    "24-hour rate: Where UK travel includes an overnight stay and incorporates breakfast, lunch and dinner a single claim of up to £66 receipted (£28 un-receipted) may be made. In these cases, individual meal rates need not be applied, and the claim will be assessed on an aggregate basis in order to allow for greater flexibility. This 24-hour rate will not apply where any one of the meals is included within a room rate, and claimants must still provide receipts for each separate meal. Only actual spend will be reimbursed and all claims remain subject to the overriding principles of reasonableness and value for money.\n",
    "\n",
    "Overseas Meals\n",
    "The cost of meals may vary significantly from country to country. Both claimants and approvers should look to the general principles of this policy therefore when incurring such costs and when determining an appropriate level of claim: in all respects expenses must be justifiable, reasonable and represent value for money.\n",
    "With this mind, the University does not look to prescribe fixed limits for overseas meals but instead provide guidance on what a reasonable claim would be.\n",
    "In the first instance, employees should expect to be reimbursed for meals that are equivalent to the standard of meal that can be purchased in the UK within the set GBP rates for UK travel detailed in sections 5.18 to 21.\n",
    "As an additional location-specific guide, employees may also consult HMRC's scale rates for overseas travel in order to judge what is reasonable when quoted in local currency. These rates do not represent an upper limit for claims (noting that they’re updated infrequently) but nor do they represent an automatic entitlement. The rates should be considered a point of reference only and all claims must uphold the general principles set out above.\n",
    "In line with section 3 above, employees will only be reimbursed for actual costs incurred as evidenced by original receipts. Where receipts for overseas subsistence cannot be obtained, employees may claim at the unreceipted GBP rates set out in sections 5.18 to 5.21 up to a maximum of £28 per day for three meals.\n",
    "\n",
    "Rented Accommodation (This section applies to GNEI staff and students only, the policy for visiting Research Fellows is detailed in Annex 4).\n",
    "Long-term assignments\n",
    "If a member of staff is working away from home on a longer-term assignment – usually for six months or more – the budget holder may require them to rent a property rather than stay in a hotel.\n",
    "The lease should be taken out in the employee’s name as they will be the sole occupant of the property; prior approval from budget holder or departmental administrator/manager must be obtained before the lease is agreed.\n",
    "The monthly rent, council tax, service charges, TV licence and utility bills (including telephone line rental) including VAT can be claimed. Alternatively, GNEI can pay the rent directly on invoice via Accounts Payable.\n",
    "If several colleagues are working away from home working on the same project and sharing a property, only one of the staff should claim expenses for the cost of the accommodation and associated bills.\n",
    "\n",
    "Relocation Expenses\n",
    "Please refer to the Relocation Expenses Guide for further details.\n",
    "\n",
    "\n",
    "\n",
    "6. Telephone and Internet costs\n",
    "This category details what can be claimed when using a telephone for business purposes.\n",
    "Always use the cheaper of either mobile or land line.\n",
    "\n",
    "GNEI mobile phone users\n",
    "As calls and line rental are paid directly by GNEI, no claims for business calls, from either a personal mobile or a home landline should be made by staff in receipt of a GNEI mobile phone.\n",
    "If it is decided that an employee requires a mobile phone for work purposes, it should be obtained via ISD’s Managed Mobile service.\n",
    "\n",
    "Own mobile phone users\n",
    "Line rental cannot be claimed. Only the cost of business calls on a call-by-call basis can be claimed, and an itemised VAT bill with the relevant calls highlighted must be provided as a receipt and attached to the claim.\n",
    "\n",
    "Home personal land line\n",
    "Only the cost of business calls on a call-by-call basis can be claimed, and an itemised VAT bill with the relevant calls highlighted must be provided as a receipt and attached to the claim.\n",
    "Any telephone line rental or standing charges cannot be claimed.\n",
    "\n",
    "Wi-Fi\n",
    "Short Wi-Fi internet session fees e.g. at airports and hotels while on GNEI business can be claimed, a receipt must be provided.\n",
    "\n",
    "Broadband/Internet Provision\n",
    "Home internet connection cannot be claimed for in any circumstances.\n",
    "\n",
    "\n",
    "\n",
    "7. Entertainment and Hospitality\n",
    "This section details what can be claimed when entertaining staff, students or external visitors – that is, providing food, drink or other hospitality.\n",
    "The nature of the event should be established:\n",
    "Staff and student entertaining: the event is to primarily entertain GNEI staff and students with some (or no) visitors.\n",
    "Business entertaining: the event is to primarily entertain visitors with some GNEI staff and students attending.\n",
    "A record of all staff and student attendees and their GNEI department names must be included with any claim – this is an HMRC requirement.\n",
    "All expenditure should be reasonable and appropriate.\n",
    "The cost of entertainment and / or hospitality should not be paid for using departmental purchasing cards under any circumstances.\n",
    "\n",
    "Hospitality for meetings\n",
    "Food can only be provided when it is necessary to hold a meeting during a normal mealtime (breakfast - before 9.00am, lunchtime - between 12.00 and 2.00pm, or evening - after 6.00pm), and it is not practical to expect staff to bring their own food. No alcohol can be provided.\n",
    "Where possible the meeting should be held on GNEI premises and food provided by GNEI's catering providers.\n",
    "\n",
    "Staff and student entertaining/hospitality\n",
    "This means food or drink for two or more members of staff or registered GNEI students in connection to GNEI business activities. It includes working lunches/dinners and team- building events. Approval from the budget holder or departmental administrator/manager must be sought before organising or paying for any staff or student entertaining.\n",
    "The cost of entertaining should not exceed £22 per head.\n",
    "\n",
    "Staff social functions\n",
    "These are events that not in connection to GNEI business activities e.g. Christmas lunches/parties, end of term socials, retirement parties etc.\n",
    "In general, staff should be expected to contribute to the cost of these events. The GNEI department contribution should be no more than £22 per head.\n",
    "Any expenditure on social functions should be approved in advance with the relevant PS Director, Faculty Director of Operations or School/PS Head of Finance.\n",
    "Costs for major staff entertaining functions (for example annual social) should be invoiced directly to GNEI, not entered on individuals' expense claims.\n",
    "\n",
    "Business entertaining\n",
    "The most senior person present should pay the bill for any business entertaining and submit the claim.\n",
    "Hospitality, where feasible, should be pre-approved by the budget holder or departmental administrator/manager.\n",
    "The cost should be appropriate and not exceed £44 per head including alcohol and service.\n",
    "If for an exceptional reason the costs exceeds £44 per head, including alcohol and service, this should be approved by the relevant PS Director, Faculty Director of Operations or School/PS Head of Finance.\n",
    "A record of all attendees and their organisation names must be kept and included with the claim; this includes any GNEI staff or students attending.\n",
    "GNEI staff or students attending a business entertaining event in a support role, e.g. greeting guests, without participating in the hospitality should not be included in the numbers attending. Treat any costs arising from their attendance as part of the cost of the function and claim them as such. \n",
    "\n",
    "\n",
    "\n",
    "8. Training\n",
    "Training and study costs can only be claimed if the training is for a business purpose.\n",
    "\n",
    "External training courses\n",
    "External training courses should be booked using a GNEI Purchase Order and paid directly by GNEI on invoice via Accounts Payable.\n",
    "Prior approval for the expense from the budget holder or departmental administrator/manager must be obtained before booking the course.\n",
    "\n",
    "Study Assistance\n",
    "The GNEI Study Assistance Scheme (SAS) provides a range of support for GNEI staff seeking to gain qualifications to support their work and careers.\n",
    "\n",
    "\n",
    "\n",
    "9. Other Expenditure\n",
    "Professional body subscriptions\n",
    "GNEI will only reimburse staff, or pay on their behalf, annual subscriptions or memberships to a professional body where either there are:\n",
    "Savings to GNEI arising from membership, for example reduced conference attendance fees, exceed the cost of membership, or\n",
    "Membership is mandatory in order to be able to teach on a professionally accredited course.\n",
    "In all cases, the professional body must feature in the list of approved organisations published by HMRC.\n",
    "\n",
    "Books, journals and magazines/periodicals\n",
    "Most books and journals should be purchased via a GNEI PO and paid directly by GNEI on invoice via Accounts Payable.\n",
    "Books should only be purchased directly by a member of staff when this is not possible, e.g. Amazon does not accept purchase orders.\n",
    "\n",
    "UK Work Permit (including renewal)\n",
    "In exceptional circumstances, and with prior approval from the relevant Dean / PS Director, Faculty Director of Operations or School/PS Head of Finance, the cost of obtaining or renewing a visa for working in the UK may be claimed through expenses.\n",
    "\n",
    "Specialist clothing\n",
    "If a job requires specialist clothing, this should usually be purchased via a GNEI PO and paid directly by GNEI on an invoice via Accounts Payable.\n",
    "\n",
    "Subject Fees\n",
    "Where individuals are paid cash to take part in a research activity/survey, the individual is required to sign a form stating that they have received the cash from the claimant. This form should be attached to the expenses claim form as a receipt.\n",
    "\n",
    "\n",
    "\n",
    "10. Advances\n",
    "Foreign Advances\n",
    "These are available for overseas travel on behalf of GNEI where other means of paying for the trip are not available.\n",
    "Staff may apply for foreign currency, traveller’s cheques or for the sterling equivalent to be paid directly into their bank account. Please go to the Foreign Advances Policy for further information.\n",
    "\n",
    "Research Advances for Subject Fees\n",
    "Staff may apply through their departments for a Research Advance for the purpose of remunerating Subjects for taking part in experiments, to a maximum of £55 cash per subject for the duration of the experiment.\n",
    "Subjects may claim in addition to this their travel (where allowed by the grant) from the advance on the production of the original receipt.\n",
    "If the Subject is to be paid more than £55 (not including travel) or they will be taking part in multiple experiments within the same research where the total is more than £55, this should be paid to the Subject directly by GNEI using a general expense claim form and not through the advance (due to HMRC tax regulations). Please go to the Research Advances Policy for further information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Annex 1 – Expense claim procedures\n",
    "How to make a claim\n",
    "All GNEI employees and students with a UK bank account must claim expenses online via the GNEIExpenses module on GNEIFinance. Anyone else should use the external visitor system.\n",
    "\n",
    "Receipts\n",
    "For GNEI employees, electronic images of original VAT receipts must be attached to all claims. Once submitted each claim will be routed automatically to the appropriate approver(s) and Accounts Payable.\n",
    "\n",
    "For students or external individuals, a claim will be processed once the approved claim form and all supporting paper receipts have reached Accounts Payable.\n",
    "\n",
    "Please see Annex 3 for further information on VAT receipts. Credit card slips or credit card/bank statements will not be accepted as evidence of business expenditure. All receipts used must include details of the goods or services purchased. \n",
    "\n",
    "If the necessary receipts or documentation is not available claimants should obtain approval from the relevant PS Director, Faculty Director of Operations or School/PS Head of Finance prior to submitting the claim, and provide evidence of this approval attached to the claim. Use the Justification field in GNEIExpenses to notify Accounts Payable that the approval is attached.\n",
    "\n",
    "Approval\n",
    "GNEI operates a system of delegated authorisation to staff and approval limits are set out in the GNEI Financial Regulations. Expense approvers should be aware of the funds available in the project to which the claim is being charged to.\n",
    "\n",
    "All expense claims require authorisation prior to payment. This authorisation is in line with GNEI’s Financial Authorised Signatory hierarchy. It is the responsibility of the claimant to ensure that the approver will confirm authorisation for each item in the claim. Claimants may be personally liable for expenses not subsequently approved.\n",
    "\n",
    "Approver(s) must personally review and then approve the claim. Approvers are expected to review claims promptly and consider the following before approving:\n",
    "The appropriateness and quantum of the expenditure\n",
    "The expenses category chosen for each item\n",
    "The project code(s) charged\n",
    "Any policy violations\n",
    "That receipts or receipt images correspond to the claim and constitute itemised receipts, not credit card slips/bank statements.\n",
    "That items within the claim meet the terms of the research funder’s terms and\n",
    "conditions (if related to sponsored research).\n",
    "\n",
    "If there are queries on any of the above, claims should be challenged by the Approver and\n",
    "rejected if incorrect.\n",
    "\n",
    "Expense claims not appropriately authorised will be returned.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Annex 2 – Exclusions\n",
    "In exceptional circumstances claims for some of these items may be allowed, prior approval should be obtained from the relevant PS Director, Faculty Director of Operations or School/PS Head of Finance. If prior approval is not obtained there is no guarantee that expenditure will be reimbursed.\n",
    "\n",
    "Broadband/Internet Provision\n",
    "Home internet connection cannot be claimed for in any circumstances.\n",
    "\n",
    "Entertainment and Hospitality\n",
    "GNEI will not pay for:\n",
    "Clothing for entertaining e.g. dress or suit hire\n",
    "Overnight accommodation\n",
    "\n",
    "Equipment\n",
    "All equipment should be purchased directly by GNEI via a Purchase Order (PO) and paid directly on invoice via Accounts Payable. Categories of equipment include, although not exclusively, IT hardware & software, laboratory equipment & consumables. GNEI have negotiated agreements with Contracted Suppliers for equipment.\n",
    "\n",
    "Fines\n",
    "GNEI does not pay:\n",
    "Fines or fixed penalties\n",
    "Administration fees charged by third parties for recovering fines\n",
    "Fines for motoring offences\n",
    "\n",
    "Gifts\n",
    "GNEI will not pay for gifts in any circumstances. Gifts for staff should be purchased using personal funds or via a staff collection. Gifts should not be claimed via expenses, nor purchased via iProcurement or a GNEI Purchasing Card.\n",
    "\n",
    "Insurance\n",
    "Travel insurance should be obtained from the Insurance section of the GNEI Finance website. Personal travel insurance policies will not cover claimants whilst abroad on GNEI’s behalf, and cannot be claimed as GNEI already provides cover for business travel. If a claim is received for travel insurance it will not be paid. Personal car/bicycle/motorcycle insurance cannot be claimed.\n",
    "\n",
    "Mobile Phone Contracts/Hardware\n",
    "Monthly contract charges and hardware cannot be claimed for. If a mobile phone is required for work purposes it should be obtained from ISD Telecoms. \n",
    "GNEI will pay for the cost of business calls made from a personal mobile phone, please see the ‘Telephone costs’ section of the GNEI Expenses Policy.\n",
    "\n",
    "Personal Expenditure\n",
    "Personal incidental costs while away on GNEI activity cannot be claimed. These include, although not exclusively, newspapers, bar drinks, mini bar, hotel video and health and fitness facilities.\n",
    "\n",
    "Stationery\n",
    "All stationery must be purchased via GNEI's approved supplier, via a PO and paid directly on invoice via Accounts Payable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Annex 3 – VAT\n",
    "VAT\n",
    "GNEI can reclaim the VAT charged on some business expenses. In order to make a proper claim GNEI needs valid receipts to provide appropriate evidence. It is the claimant’s responsibility to obtain a receipt showing VAT where it applies.\n",
    "\n",
    "As defined by HMRC, original VAT receipts must accompany all claims. Credit card slips or credit card/bank statements will not be accepted as evidence of expenditure. All receipts should show the following information:\n",
    "Name, address and VAT number of the supplier\n",
    "Date of supply and date of issue if different\n",
    "Description of the goods or services\n",
    "For each VAT rate the total amount payable including VAT and the VAT rate charged\n",
    "Name and address of person to whom the services are supplied (if appropriate)\n",
    "Unit price (if the supply cannot be broken down into countable elements then the total tax exclusive price will be the unit price)\n",
    "The total amount of VAT charged\n",
    "\n",
    "\n",
    "\n",
    "Annex 4 - Visiting Research Fellows Subsistence Payments\n",
    "Subsistence payments should be claimed via a Visiting Research Fellow form, which will be processed as a manual  expense form if the following conditions are met:\n",
    "The general rule is to determine if an overseas visitor is an employee or not. If the overseas visitor does not undertake any duties (other than research) normally performed by GNEI staff and they are not subject to the usual terms and conditions of employment, then payments may be made to them without deduction of PAYE and NI.\n",
    "Usual terms and conditions of employment would include such matters as the ability of GNEI to determine hours worked, location of work, to set a probationary period and entitlement of the payee to holiday pay, sick pay and to join a pension scheme.\n",
    "\n",
    "The individual must be visiting for less than two years. \n",
    "\n",
    "Subsistence payments may be made gross on the Visiting Research Fellow form if the following conditions are met:\n",
    "There is no contract of employment.\n",
    "There is no contract for services, whether written or verbal.\n",
    "The visiting Research Fellow does not undertake any duties (apart from research) normally performed by staff of GNEI.\n",
    "The grant is paid to cover living and travel expenses.\n",
    "The visiting Research Fellow is here for a maximum of two years.\n",
    "The visiting Research Fellow form must clearly state that the payment is for travel and subsistence of an overseas visiting researcher.\n",
    "The individual’s home country and institution address is included on the form.\n",
    "The source of funding is stated on the form.\n",
    "If the visiting Research Fellow does undertake any teaching duties or hold seminars, then a proportion of the payments made to them must be attributed to these duties. Payments may still be made gross but they must be reported to the HMRC. Details of all such payments in any tax year (6 April to 5 April) must be returned to the Taxation and Commercial Accountant in Finance by 30 April each year.\n",
    "\n",
    "\n",
    "Annex 5 - Childcare and Caring Responsibility costs\n",
    "Various funders now allow childcare or caring responsibility costs to be claimed as allowable expenses if the costs are incurred for working outside normal working patterns (e.g. childcare or eldercare costs while the primary carer attends a conference outside usual contracted hours).\n",
    "\n",
    "Please refer to your funder’s terms and conditions or obtain permission from them prior to incurring the expenditure. If you require further guidance please contact Award Services, Research and Innovation Services award-services@gnei.ac.uk\n",
    "\n",
    "Criteria\n",
    "The following criteria will need to be met for claims to be approved through GNEIExpenses:\n",
    "Staff must be on a grant under the terms of which such a claim is expressly allowable and eligible for reimbursement.\n",
    "There must be an auditable record such as conference attendance confirmation and receipts for the cost of the childcare / caring responsibility.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(len(context_full))\n",
    "\n",
    "context_travel = \"\"\"\n",
    "\n",
    "GNEI Expenses Policy\n",
    "1. Introduction \n",
    "This Expenses Policy (‘the Policy’) provides guidance to all individuals claiming reimbursement of reasonable expenses incurred in connection with GNEI business.\n",
    "\n",
    "2. General Principles\n",
    "GNEI employees and all others engaged in GNEI activity (collectively claimants) will be reimbursed for the actual cost of expenses incurred, wholly, exclusively and necessarily in the performance of GNEI activity as prescribed in this policy.\n",
    "The expense must be justifiable and reasonable according to the information in this policy, and the claim should always be prepared honestly, legally and responsibly. Any breach of the policy could lead to disciplinary action, up to and including dismissal.\n",
    "Whilst this policy aims to provide comprehensive guidance on reimbursable out-of- pocket expenditure, it is recommended that staff still seek advance approval from their budget holder or department manager in any situation where interpretation of this policy is in doubt.\n",
    "Employees should be able to procure the majority of goods and services required for GNEI business using the standard purchase to payment procedures using the GNEIFinance system.\n",
    "It is the responsibility of authorised signatories to ensure that this expenses policy is upheld; and it should be noted that individual claims falling outside of the policy may be subject to review and/or rejection by Finance.\n",
    "Electronic images of original receipts must accompany all claims. Credit card slips or credit card/bank statements will not be accepted as evidence of business expenditure. All receipts used should include details of what goods or services have been purchased. Original paper receipts do not need to be stored locally, unless they relate to sponsored research and it is explicit in the terms of the grant that original paper receipts need to be retained.\n",
    "When making a claim for a group, the most senior person present should pay for the expenditure and make the claim.\n",
    "Expenses should be submitted as soon as possible after they have been incurred. Claims must be made within three months. Approval should be gained from the relevant School or PS Head of Finance if the claim is over three months old.\n",
    "It costs a fixed amount to process any claim, so claimants should endeavour not to submit claims for less than £33, unless they represent the total of expenses in a three-month period.\n",
    "If a claimant inadvertently makes an error with an expense claim and has been reimbursed, GNEI will recover the amount from them.\n",
    "Expenses must not be used as a way of rewarding people or encouraging them to work in remote locations.\n",
    "GNEI will not meet the cost of expenses for spouses, partners, other family members or friends of GNEI staff.\n",
    "Claims are checked by Accounts Payable staff and any deemed to be fraudulent will be investigated and referred to GNEI’s Internal Auditors. If a claimant is found to have submitted a fraudulent claim they will face disciplinary proceedings, leading to penalties up to and including summary dismissal.\n",
    "\n",
    "\n",
    "\n",
    "3. Travel\n",
    "This section details what can be claimed as travel expenses and provides information on specific modes of transport and other expenses associated with travel.\n",
    "\n",
    "General guidance for travelling on GNEI business\n",
    "The cost of business travel, that is, journeys away from your normal place of work while undertaking GNEI business can be claimed.\n",
    "In line with the GNEI travel policy, all business trips in the UK or abroad must be authorised in writing in advance by the GNEI budget holder responsible for making the funds available. It is recommended that this is attached to the claim.\n",
    "The budget holder / manager must agree that the travel is necessary and that alternative methods such as conference call, video conference, phone or email cannot be used or are not appropriate. Consideration should also be given as to how to minimise the environmental impact of travel.\n",
    "This authorisation ensures that anticipated costs are in line with departmental plans, are eligible under the terms of any associated funding, and that subsequent expense claims can be submitted with confidence, subject to the provisions of this policy.\n",
    "GNEI has preferred travel suppliers for airline, hotel and car hire bookings details on these suppliers can be found on the Procurement website. These suppliers should be used for travel bookings where possible.\n",
    "\n",
    "Private travel costs\n",
    "The following cannot be claimed:\n",
    "Travel between home and normal place of work.\n",
    "Business travel broadly similar to the claimant’s normal commute.\n",
    "Recreational travel and accommodation at or near the business travel destination.\n",
    "Travel or accommodation for family or friends accompanying the claimant on the business journey.\n",
    "\n",
    "Rate of exchange\n",
    "If a credit card was used to pay for overseas expenses, the rate charged by the credit card issuer can be claimed. A copy of the statement should be used to evidence the rate used.\n",
    "For employees GNEIExpenses will use a default rate for translating foreign currency transactions; this can be overridden if different to the rate at the time of expenditure and fluctuations will be allowed within a set tolerance. If the automatic rate is overridden, a copy of the bank statement should be included with the claim to back up the rate used.\n",
    "Buying foreign currency is not a genuine business expense and cannot be claimed.\n",
    "\n",
    "Travel Insurance\n",
    "Travel Insurance needs to be activated before traveling overseas, travelling in the UK for fieldtrips or travelling on a business trip where there is an overnight stay or air travel involved.\n",
    "Insurance can be activated by completing a travel insurance request form, which can be found in the Insurance Section on the Finance website.\n",
    "The Insurance team will issue a cover note and provide details of the policy. There is no cost to departments for travel insurance. For regular travellers the Insurance Section can issue annual cover notes on request.\n",
    "If a claimant has an existing medical condition and is not travelling against their doctors’ orders, this is covered by the GNEI Travel Insurance Policy at no extra cost.\n",
    "Personal travel insurance cannot be claimed as GNEI already provides cover for business travel, please refer to Annex 2 for further information on this exclusion.\n",
    "\n",
    "Public Transport\n",
    "Journeys made on rail, bus, ferry, river boat, underground, metro or tram services – can be claimed.\n",
    "The most economical method of travel should be used. For example, if the claimant uses public transport in London regularly, they should use an Oyster card rather than paying for single journeys by cash.\n",
    "\n",
    "Train journeys\n",
    "These should be standard class unless a heavily discounted first-class ticket is booked significantly in advance of the date of travel.\n",
    "If the claimant does travel first class, an explanation must be included on the claim and if necessary the claim will be passed to the relevant School or PS Head of Finance for approval.\n",
    "Trains should be booked in advance where possible via GNEI's approved travel provider or similar rail travel provider as this is the most cost effective option. Tickets can then be collected at the relevant station or sent via post.\n",
    "\n",
    "Tube and bus journeys in London\n",
    "If a London Transport Oyster or contactless credit/debit card is used for business journeys, the details of each individual journey should be entered on the claim, being specific about locations, date and times and purpose of journey.\n",
    "Only the relevant journey should be claimed, not the whole top-up cost, unless the whole top-up will be used for GNEI business travel. Receipts are not necessary for individual journeys but are required if claiming for the whole top- up fee.\n",
    "\n",
    "Receipts\n",
    "Relevant tickets or documentation should be kept and attached to the expense claim electronically.\n",
    "Claimants who have a rail or Oyster season ticket enabled to use the underground can only claim for travel if extra expenses are incurred that are not covered by the season ticket and can provide a receipt as proof of the expense.\n",
    "Ticket machines will provide receipts if the ticket is retained by the automatic barrier at the end of a journey.\n",
    "\n",
    "Taxis\n",
    "Taxi fares can only be claimed for journeys where:\n",
    "It is clearly the most efficient and cost-effective method of transport.\n",
    "Alternative methods of transport are impractical due to pregnancy, disability, illness or injury, luggage or similar.\n",
    "A member of staff is working in the office very late, i.e. after 11pm.\n",
    "Prior approval has been given, where feasible, from the budget holder or departmental administrator/manager.\n",
    "\n",
    "Business Mileage\n",
    "If a car, motorcycle or bicycle has been used on a business trip the costs can be claimed using a mileage allowance – that is, a sum of money for every business mile travelled. GNEI does not reimburse claims based on actual petrol receipts.\n",
    "GNEI has standard rates for these allowances, depending on the circumstances surrounding the trip (see below).\n",
    "Only additional costs incurred, above what would normally be spent for commuting to work (which are not eligible as a business expense) can be claimed.\n",
    "A car should only be used if it is cheaper and more efficient than public transport. Cars should be shared with other staff members wherever possible.\n",
    "The claim should state the start and end points of the journey, along with the number of miles being claimed.\n",
    "Mileage is paid at the HMRC approved mileage allowance rates:\n",
    "Cars - 50p per mile for the first 11,000 miles in a tax year and 28p per mile thereafter.\n",
    "Motorcycles - 26p per mile.\n",
    "Bicycles - 22p per mile.\n",
    "\n",
    "Car/bicycle/motorcycle insurance\n",
    "It is important that adequate personal insurance cover for business use is in place, as cars, bicycles or motorcycles belonging to claimants are not covered by GNEI’s insurance. Personal insurance cannot be claimed, see Annex 2 for further information.\n",
    "If a staff member uses or allows another member of staff to use a vehicle on GNEI’s business that is insured for third party risks only, GNEI will not be held liable for any of the following:\n",
    "damage or repairs to the vehicle\n",
    "property lost or damaged\n",
    "personal injuries sustained or caused during or as a result of such use.\n",
    "\n",
    "Car sharing\n",
    "Only the driver can claim mileage for a journey where two or more GNEI staff share a car. The following cannot be claimed:\n",
    "Insurance cover for private cars.\n",
    "Accident or breakdown recovery costs, such as AA membership.\n",
    "\n",
    "Air Travel\n",
    "Air travel should be booked using GNEI’s preferred travel provider where possible.\n",
    "GNEI prefers lower fares, based on flight arrival and departure times which cannot be changed. Where there is a need for flexibility in departure or arrival times, a different ticket option will have to be requested. Flexible tickets for flights under three hours require approval by the relevant Dean / PS Director, Faculty Director of Operations or School/PS Head of Finance.\n",
    "Membership of a frequent traveller scheme and potential air miles benefits must not deflect from using the most cost-effective airline option for travel.\n",
    "GNEI does not meet the travelling costs of:\n",
    "Employees of contractors\n",
    "Spouses, partners, other family members or friends.\n",
    "Classes of travel:\n",
    "For flights of five hours or less, staff should book restricted tickets in economy.\n",
    "For flights lasting five hours or longer staff may travel premium economy.\n",
    "Where a long international flight is immediately followed by a presentation or meeting, staff may travel by business class. This must be pre-approved by the relevant Dean / PS Director, Faculty Director of Operations or School/PS Head of Finance.\n",
    "\n",
    "Toll charges\n",
    "Toll charges paid for in the course of a business journey can be claimed. Toll bridges or roads rarely give receipts; therefore the name of the toll bridge or road should be included on the expense claim.\n",
    "\n",
    "Congestion charges\n",
    "GNEI will not reimburse the cost of any congestion charge for staff or visitors except in exceptional circumstances, e.g. where a member of staff has to collect/deliver equipment to a location and the journey could not be completed on public transport. This must be approved in advance by the relevant School/PS Head of Finance.\n",
    "\n",
    "Car Parking\n",
    "Reasonable parking costs on business visits and journeys away from the office can be claimed.\n",
    "Hotel parking costs can be claimed if they are charged separately on the bill.\n",
    "If a claimant has to drive to their place of work as part of a business journey, e.g. to collect equipment, parking costs can be claimed. Details of the reason should be included with the claim.\n",
    "The cost of parking at an entertainment venue cannot be claimed.\n",
    "VAT can be reclaimed (where it's charged) on parking fees which cost less than £28 even if the claimant is not able to get a VAT receipt.\n",
    "\n",
    "Car Hire\n",
    "Approval must be obtained from the budget holder or departmental administrator/manager before a car is hired.\n",
    "If hiring a car is the cheapest mode of travel, it should be booked through GNEI's approved car hire provider (for UK or overseas).\n",
    "The car should only be used for business purposes.\n",
    "Rates are inclusive of CDW (Collision Damage Waiver), TPI (Third Party Insurance) and PAI (Personal Accident Insurance).\n",
    "Any petrol bought solely for business purposes can be claimed. A VAT receipt should be obtained for purchases, a credit card receipt is not sufficient.\n",
    "\n",
    "Group Travel\n",
    "This is in relation to a staff conference or student field trip. Prior approval from the budget holder or departmental administrator/manager should be obtained before booking a group trip.\n",
    "The same rules apply for each category of travel and overnight costs as for travelling as an individual.\n",
    "All travel (including coach hire) and overnight costs should be booked in advance.\n",
    "GNEI's approved travel provider provides a group travel & conference booking facility, where all elements of the trip can be arranged. This can be used for arranging a business conference, as well as student field trips.\n",
    "Please ensure adequate insurance is in place for all staff and students on the trip. Travel insurance needs to be activated prior to the trip by using the Insurance section of the Finance website.\n",
    "\n",
    "Travel Incidentals\n",
    "Claimants can claim the following goods or services if they are related to the business purpose of the trip:\n",
    "The cost of obtaining a visa for working abroad on GNEI business. These can be obtained from GNEI's approved travel provider.\n",
    "The cost of any vaccinations needed for working abroad.\n",
    "There may be occasions when a claimant needs a second passport, for example when travel to one country could lead to immigration difficulties in travelling on business to some other countries. GNEI will pay this expense.\n",
    "Approval must be obtained in advance from the budget holder or departmental administrator/manager.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "context_accom = \"\"\"\n",
    "\n",
    "3. General Principles\n",
    "GNEI employees and all others engaged in GNEI activity (collectively claimants) will be reimbursed for the actual cost of expenses incurred, wholly, exclusively and necessarily in the performance of GNEI activity as prescribed in this policy.\n",
    "The expense must be justifiable and reasonable according to the information in this policy, and the claim should always be prepared honestly, legally and responsibly. Any breach of the policy could lead to disciplinary action, up to and including dismissal.\n",
    "Whilst this policy aims to provide comprehensive guidance on reimbursable out-of- pocket expenditure, it is recommended that staff still seek advance approval from their budget holder or department manager in any situation where interpretation of this policy is in doubt.\n",
    "Employees should be able to procure the majority of goods and services required for GNEI business using the standard purchase to payment procedures using the GNEIFinance system.\n",
    "It is the responsibility of authorised signatories to ensure that this expenses policy is upheld; and it should be noted that individual claims falling outside of the policy may be subject to review and/or rejection by Finance.\n",
    "Electronic images of original receipts must accompany all claims. Credit card slips or credit card/bank statements will not be accepted as evidence of business expenditure. All receipts used should include details of what goods or services have been purchased. Original paper receipts do not need to be stored locally, unless they relate to sponsored research and it is explicit in the terms of the grant that original paper receipts need to be retained.\n",
    "When making a claim for a group, the most senior person present should pay for the expenditure and make the claim.\n",
    "Expenses should be submitted as soon as possible after they have been incurred. Claims must be made within three months. Approval should be gained from the relevant School or PS Head of Finance if the claim is over three months old.\n",
    "It costs a fixed amount to process any claim, so claimants should endeavour not to submit claims for less than £33, unless they represent the total of expenses in a three-month period.\n",
    "If a claimant inadvertently makes an error with an expense claim and has been reimbursed, GNEI will recover the amount from them.\n",
    "Expenses must not be used as a way of rewarding people or encouraging them to work in remote locations.\n",
    "GNEI will not meet the cost of expenses for spouses, partners, other family members or friends of GNEI staff.\n",
    "Claims are checked by Accounts Payable staff and any deemed to be fraudulent will be investigated and referred to GNEI’s Internal Auditors. If a claimant is found to have submitted a fraudulent claim they will face disciplinary proceedings, leading to penalties up to and including summary dismissal.\n",
    "\n",
    "5. Overnight costs and allowances\n",
    "Such costs are subject to the requirement for pre-approval in section 4.3 above.\n",
    "\n",
    "Hotels\n",
    "In line with the GNEI travel policy, Hotels must be booked via GNEI's approved travel provider for both overseas and UK travel, with recovery through expenses only permitted in those cases set out in Section 3.1 of that policy and subject to an appropriate level of advance approval as set out in section 3.2. The approval obtained must be attached to submitted expense claims alongside receipts for the room cost, breakfast, dinner and other eligible incidentals.\n",
    "Hotel city tax: from time to time, additional local city tax charges may apply for accommodation. These can be reclaimed via expenses.\n",
    "\n",
    "UK Hotels\n",
    "As stipulated by HMRC, the cost of accommodation at or near the normal place of work does not qualify as business travel; any claim would be subject to tax on the individual as a reimbursement of personal expenditure. Such costs may not be claimed, and staff are asked to bear this in mind when making their travel arrangements.\n",
    "The cost of a hotel in the UK should not exceed £210 including VAT (room only) in London or £140 including VAT (room only) outside of London. If a hotel cost is higher than these limits, this should be pre-approved by the relevant Dean / PS Director, Faculty Director of Operations or School/PS Head of Finance.\n",
    "\n",
    "Overseas hotels\n",
    "The cost of overseas accommodation may vary significantly from country to country. Both claimants and approvers should look to the general principles of this policy therefore when incurring such costs and when determining an appropriate level of claim: in all respects expenses must be justifiable, reasonable and represent value for money.\n",
    "With this in mind, the University does not look to prescribe fixed limits for overseas accommodation but instead provide guidance on what a reasonable claim would be so that claimants and approvers are able to make a fully informed assessment.\n",
    "In the first instance, employees should expect to be reimbursed for hotels that are equivalent to the most appropriate GBP rate for UK travel detailed in section 5.5. For overseas stays the expectation of the University is that an employee should book accommodation that is equivalent to a UK 3-star level. For areas where there is deprivation or a high security risk then the rating may be increased to 4-star.\n",
    "Employees may also consult HMRC's scale rates for overseas travel in order to judge what is reasonable when quoted in local currency. These rates do not represent an upper limit nor an automatic entitlement but should be considered a point of reference only.\n",
    "Employees should be aware however that claims materially in excess of the HMRC rates are likely to be subject to challenge either during the approval process or before settlement by the central processing teams. Where this is expected to be the case, claimants should ensure that clear justification and confirmation of pre-approval from the relevant budget holder is attached to the claim as per Section 4.3.\n",
    "Where attendance at an international conference requires specific accommodation (for example by virtue of a fixed attendance package or as a necessary means of realising the benefits of the conference), this factor should also be documented with pre-approval attached to the claim.\n",
    "Incidentals\n",
    "All necessary incidentals (including brief personal calls home) should be receipted and claimed:\n",
    "Newspapers, bar drinks, mini bar, hotel video and health and fitness facilities cannot be claimed.\n",
    "Laundry costs can only be claimed if absolutely necessary, the cost is reasonable and appropriate and the stay away is for at least 5 consecutive nights.\n",
    "\n",
    "Telephone, Fax and Internet Charges\n",
    "The cost of business calls, fax and internet access charges for business use, as long as they are included on an itemised bill, can be claimed.\n",
    "When working overseas a telephone card can be purchased as this reduces the cost of local calls, the cost of this instead of calls from hotels can be claimed.\n",
    "\n",
    "Meals\n",
    "When travelling on GNEI business, employees are entitled to recover the cost of reasonable subsistence, namely breakfast, lunch, and dinner. Meals cannot be claimed if already included in hotel accommodation rates being claimed for.\n",
    "If several members of staff have a meal together, the most senior person should pay and submit the expense claim. In such a case no-one other than the most senior person should claim an allowance for the meals. Please ensure that it is made clear in the justification field of GNEIExpenses that the receipt is covering more than one employee’s meal, and each employee name (covered by the receipt) should be listed.\n",
    "The following rates include the cost of alcoholic beverages.\n",
    "\n",
    "UK Meals\n",
    "Breakfast. If breakfast is not included in the hotel room rate, the cost can be claimed, which should be receipted and claimed as part of the overnight stay to a maximum of £12 including VAT and service. If it is not possible to obtain a receipt, then £5.5 may be claimed for breakfast.\n",
    "Lunch. If the claimant is working away from their normal place of work, lunch can be claimed to a maximum of £18 including VAT and service. If it is not possible to obtain a receipt, then £5.5 may be claimed for lunch.\n",
    "Evening meal. The cost of an evening meal can be claimed which should be receipted and claimed as part of the overnight stay to a maximum of £36 including VAT and service. If it is not possible to obtain a receipt, then £17 may be claimed for dinner.\n",
    "24-hour rate: Where UK travel includes an overnight stay and incorporates breakfast, lunch and dinner a single claim of up to £66 receipted (£28 un-receipted) may be made. In these cases, individual meal rates need not be applied, and the claim will be assessed on an aggregate basis in order to allow for greater flexibility. This 24-hour rate will not apply where any one of the meals is included within a room rate, and claimants must still provide receipts for each separate meal. Only actual spend will be reimbursed and all claims remain subject to the overriding principles of reasonableness and value for money.\n",
    "\n",
    "Overseas Meals\n",
    "The cost of meals may vary significantly from country to country. Both claimants and approvers should look to the general principles of this policy therefore when incurring such costs and when determining an appropriate level of claim: in all respects expenses must be justifiable, reasonable and represent value for money.\n",
    "With this mind, the University does not look to prescribe fixed limits for overseas meals but instead provide guidance on what a reasonable claim would be.\n",
    "In the first instance, employees should expect to be reimbursed for meals that are equivalent to the standard of meal that can be purchased in the UK within the set GBP rates for UK travel detailed in sections 5.18 to 21.\n",
    "As an additional location-specific guide, employees may also consult HMRC's scale rates for overseas travel in order to judge what is reasonable when quoted in local currency. These rates do not represent an upper limit for claims (noting that they’re updated infrequently) but nor do they represent an automatic entitlement. The rates should be considered a point of reference only and all claims must uphold the general principles set out above.\n",
    "In line with section 3 above, employees will only be reimbursed for actual costs incurred as evidenced by original receipts. Where receipts for overseas subsistence cannot be obtained, employees may claim at the unreceipted GBP rates set out in sections 5.18 to 5.21 up to a maximum of £28 per day for three meals.\n",
    "\n",
    "Rented Accommodation (This section applies to GNEI staff and students only, the policy for visiting Research Fellows is detailed in Annex 4).\n",
    "Long-term assignments\n",
    "If a member of staff is working away from home on a longer-term assignment – usually for six months or more – the budget holder may require them to rent a property rather than stay in a hotel.\n",
    "The lease should be taken out in the employee’s name as they will be the sole occupant of the property; prior approval from budget holder or departmental administrator/manager must be obtained before the lease is agreed.\n",
    "The monthly rent, council tax, service charges, TV licence and utility bills (including telephone line rental) including VAT can be claimed. Alternatively, GNEI can pay the rent directly on invoice via Accounts Payable.\n",
    "If several colleagues are working away from home working on the same project and sharing a property, only one of the staff should claim expenses for the cost of the accommodation and associated bills.\n",
    "\n",
    "Relocation Expenses\n",
    "Please refer to the Relocation Expenses Guide for further details.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(len(context_accom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_format(example, context):\n",
    "    # Format system\n",
    "    system1 = f\"\"\"A <<<POLICY>>> and a <<<SCENARIO>>> will be provided, you are GREAT at analysing the <<<SCENARIO>>> according to the <<<POLICY>>>. Your answer MUST and and CAN ONLY be a valid json format, having 4 text fields: Classification T1, Reimbursement Amount, Classification T2, and Reasons. Your answer should follow these structures: \\n 1) \"Classification T1\" :you MUST choose 'Policy Violated' or 'Policy Not Violated' for the activity in the <<<scenario>>> \\n \\n 2) \"Reimbursement Amount\" : Answer One number directly that according to the Policy the total amount of money can be reimbursed \\n 3) \"Classification T2\" : Compare to the requested reimbursement amount in the scenario, and answer if the claim is 'Fully Reimbursable'/'Partially Reimbursable'/'Not Reimbursable'/'Further Clarification Required'. \\n 4) give reasons for your choices according to the <<<POLICY>>>.\\n\\n\n",
    "    \"\"\"\n",
    "\n",
    "    system2 = f\"\"\"<<<<<Policy Start:{context}\n",
    "    \\n\\n\\nPolicy End>>>>>\\n\\n\n",
    "    \"\"\"\n",
    "\n",
    "    # Format instruction\n",
    "    prompt = f\"\"\"<<<<<Scenario Start:\\n\\n{example['Prompt']}\\n\\nScenario End>>>>>\"\"\"\n",
    "\n",
    "    # Format chosen answer\n",
    "    chosen = f\"\"\"Answer: {{\"Classification T1\": {example['Classification T1']},\n",
    "    \"Reimbursement Amount\": £{example['Amount']},\n",
    "    \"Classsifcation T2\": {example['Classification T2']},\n",
    "    \"Reasons\": ...}} \n",
    "    <eos>\\n\"\"\"\n",
    "\n",
    "    # Format rejected answer\n",
    "    rejected = f\"\"\"Answer: {{\"Classification T1\": {example['rejected_T1']},\n",
    "    \"Reimbursement Amount\": £{example['rejected_Amount']},\n",
    "    \"Classsifcation T2\": {example['rejected_T2']},}} \n",
    "    <eos>\\n\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": system1 + system2 + prompt,   # sequence should follow inference patterns and context generally come before related questions\n",
    "        # \"prompt\": system1 + prompt,           # option for non-policy ft\n",
    "        \"chosen\": chosen,\n",
    "        # \"rejected\":\"\",\n",
    "        \"rejected\":rejected,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 5068.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representing GNEI, I journeyed from London to Madrid for a conference, incurring a flight expense of £92. Furthermore, the bank levied a £15 charge for currency exchange during the transaction. Could you please confirm the reimbursement amount?\n",
      "Answer: {\"Classification T1\": Policy Not Violated,\n",
      "    \"Reimbursement Amount\": £750.0,\n",
      "    \"Classsifcation T2\": Fully Reimbursable,\n",
      "    \"Reasons\": ...} \n",
      "    <eos>\n",
      "\n",
      "Answer: {\"Classification T1\": Policy Violated,\n",
      "    \"Reimbursement Amount\": £432.68,\n",
      "    \"Classsifcation T2\": Partially Reimbursable,} \n",
      "    <eos>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 190\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save columns\n",
    "original_columns = tr_data.column_names\n",
    "\n",
    "# Format dataset\n",
    "dataset = tr_data.map(\n",
    "    lambda x: data_format(x,context = context_accom),\n",
    "    remove_columns=original_columns # remove un-used columns for training process\n",
    ")\n",
    "\n",
    "print(data_frame['Prompt'][5])\n",
    "print(dataset[5]['chosen'])\n",
    "print(dataset[5]['rejected'])\n",
    "dataset = dataset.train_test_split(test_size=0.05)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame = pd.read_excel(file_path,sheet_name=2, engine='openpyxl')\n",
    "\n",
    "data_test = test_frame[test_frame[\"Category\"] == \"Accomodation and Sustenance\"]\n",
    "data_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(prompt_num, data_frame, context):\n",
    "    \"\"\"\n",
    "        prompt_num: int: how many data points in data_frame involved\n",
    "        data_frame: pdDataframe: provides a \"Prompt\" column containing scenarios\n",
    "        context:    str: the policy script to be included in the prompt\n",
    "\n",
    "        return:\n",
    "        pdDataframe: model_original_resp: a df containing trimmed responses in a column manner\n",
    "        \"\"\"\n",
    "    model_original_resp = []\n",
    "    for i in range(prompt_num):\n",
    "        # iterating through all data points\n",
    " \n",
    "        prompt = f\"\"\"A <<<POLICY>>> and a <<<SCENARIO>>> will be provided, you are GREAT at analysing the <<<SCENARIO>>> according to the <<<POLICY>>>. Your answer MUST and and CAN ONLY be a valid json format, having 4 text fields: Classification T1, Reimbursement Amount, Classification T2, and Reasons. Your answer should follow these structures: \\n 1) \"Classification T1\" :you MUST choose 'Policy Violated' or 'Policy Not Violated' for the activity in the <<<scenario>>> \\n \\n 2) \"Reimbursement Amount\" : Answer One number directly that according to the Policy the total amount of money can be reimbursed \\n 3) \"Classification T2\" : Compare to the requested reimbursement amount in the scenario, and answer if the claim is 'Fully Reimbursable'/'Partially Reimbursable'/'Not Reimbursable'/'Further Clarification Required'. \\n 4) give reasons for your choices according to the <<<POLICY>>>.\\n\\n\n",
    "\n",
    "        <<<<<start examples:\n",
    "\n",
    "        <<<<<example one:\n",
    "        scenario: I used a TFL travel card to travel from home to the GNEI work station every day for a week and it cost me £50. How much can I reimburse?\n",
    "\n",
    "        Answer:{{\"Classification T1\" : \"Policy Violated\",\n",
    "                \"Reimbursement Amount\" : £0,\n",
    "                \"Classification T2\" : \"Not Reimbursable\",\n",
    "                \"Reasons\":\"According to the GNEI Expenses Policy, travel between home and normal place of work (i.e., the GNEI campus) cannot be claimed. Thus, this expense cannot be reimbursed.\"}};\n",
    "\n",
    "        <<<<<example two:\n",
    "        scenario: I booked a taxi from the GNEI work station to my home yesterday night. The total cost was £25. I left the campus at 11:30pm. Can I expense this charge?\n",
    "\n",
    "        Answer:{{\"Classification T1\" : \"Policy Not Violated\",\n",
    "                \"Reimbursement Amount\" : £25,\n",
    "                \"Classification T2\" : \"Fully Reimbursable\",\n",
    "                \"Reasons\":\"The GNEI Expenses Policy states that taxi fares can be claimed for journeys where a member of staff is working in the office very late, specifically after 11pm​​. Therefore, your taxi fare under these circumstances falls within the allowable expenses according to the policy, and total reimbursable amount is £25.\"}};\n",
    "        \n",
    "        <<<<<example three:\n",
    "        scenario: I booked a taxi from the GNEI campus to my home yesterday night. The total cost was £35. I left the campus at 9:30pm. Can I expense this charge?\n",
    "\n",
    "        Answer:{{\"Classification T1\" : \"Policy Violated\",\n",
    "                \"Reimbursement Amount\" : £0,\n",
    "                \"Classification T2\" : \"Further Clarification Required\",\n",
    "                \"Reasons\":\"Based on the GNEI Expenses Policy, taxi fares can only be claimed for specific reasons, such as if a member of staff is working in the office very late, specifically mentioned as after 11pm. Since you left the campus at 9:30pm, your taxi fare does not meet the criteria set out under the \"Taxis\" section for allowable taxi expenses.\n",
    "                Therefore, based on the information provided in the GNEI Expenses Policy, you cannot expense the £35 taxi charge for a trip from the GNEI campus to your home that occurred at 9:30pm.\"}};\n",
    "        \n",
    "        <<<<<example four:\n",
    "        scenario: I drove to Cambridge for a business trip along with another GNEI colleagues. The total distance covered was 60 miles. I accidentally hit the sidewalk and damaged the side of the car. The total cost of the repairs was £100. How much can I claim in expenses?  \n",
    "\n",
    "        Answer:{{\"Classification T1\" : \"Policy Violated\",\n",
    "                \"Reimbursement Amount\" : £30,\n",
    "                \"Classification T2\" : \"Partially Reimbursable\",\n",
    "                \"Reasons\":\"For your business trip to Cambridge, driving a car for a total distance of 60 miles with another GNEI colleagues, you can claim expenses based on the GNEI Expenses Policy as follows:\n",
    "                - **Mileage Claim**: According to section **4.32**, mileage for cars is reimbursed at a rate of 50p per mile for the first 11,000 miles in a tax year. Therefore, for 65 miles, the claim would be 60 miles x £0.50/mile = £30.\n",
    "                - **Damage to the Vehicle**: Section **4.34** specifies that GNEI will not be held liable for damage or repairs to the vehicle used on GNEI’s business. Thus, the cost of repairs due to the accident, totaling £100, cannot be claimed under the policy.\n",
    "                **Documentation and Approvals Required:**\n",
    "                1. **Mileage Documentation**: Include details such as the start and end points of the journey, the purpose of the business trip, and the total number of miles claimed.\n",
    "                2. **Approval for Business Travel**: Ensure that the business trip was pre-approved as per GNEI's travel policy requirements.\n",
    "                3. **Vehicle Damage**: Since the policy excludes claims for vehicle damage or repairs, there is no requirement for documentation or approval in this context.\n",
    "\n",
    "                **Total Amount That Can Be Expensed: £30** (for mileage only, as vehicle repair costs are not covered).\"}};\n",
    "\n",
    "        examples End>>>>>\n",
    "\n",
    "        <<<<<Policy Start:{context}\n",
    "        \\n\\n\\nPolicy End>>>>>\\n\\n<<<<<Scenario Start:\\n\\n{data_frame['Prompt'][i]}\\n\\nScenario End>>>>>\"\"\"\n",
    "        \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        # input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        # print(\"tokenization completed\",input_ids['input_ids'].shape)\n",
    "\n",
    "        #get \"Answer\" token ID in the tokenizer, forcing the model to start answering with \"Answer\" and thus give a json output\n",
    "        start_token_id = tokenizer.convert_tokens_to_ids(\"Answer\")\n",
    "       \n",
    "        outputs = model.generate(**input_ids,max_new_tokens = 400, min_new_tokens = 5,\n",
    "                                 #do_sample = False, temperature = 5, top_k = 0.9, \n",
    "                                 repetition_penalty = 1.2,\n",
    "                                 forced_bos_token_id = start_token_id) # alter max number of tokens here\n",
    "        \n",
    "        # trim the output by removing prompt\n",
    "        model_response = tokenizer.decode(outputs[0])\n",
    "\n",
    "        trimmed_output = model_response[len(prompt)+5:]\n",
    "\n",
    "        # collecting responses\n",
    "        model_original_resp.append(trimmed_output)\n",
    "\n",
    "        if ((i+1)%10 == 0):\n",
    "                print(f\"the\",(i+1),\"th prompt responded\")\n",
    "        #print(f\"the\",(i+1),\"th prompt responded\")\n",
    "    return model_original_resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m composed_resp \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     model_original_resp \u001b[38;5;241m=\u001b[39m \u001b[43mget_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_accom\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      5\u001b[0m     composed_resp\u001b[38;5;241m.\u001b[39mextend(model_original_resp)\n\u001b[1;32m      7\u001b[0m df_original_resp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(composed_resp, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal response\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[9], line 71\u001b[0m, in \u001b[0;36mget_responses\u001b[0;34m(prompt_num, data_frame, context)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# print(\"tokenization completed\",input_ids['input_ids'].shape)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#get \"Answer\" token ID in the tokenizer, forcing the model to start answering with \"Answer\" and thus give a json output\u001b[39;00m\n\u001b[1;32m     69\u001b[0m start_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m#do_sample = False, temperature = 5, top_k = 0.9, \u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_token_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# alter max number of tokens here\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# trim the output by removing prompt\u001b[39;00m\n\u001b[1;32m     77\u001b[0m model_response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2417\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m   2416\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[0;32m-> 2417\u001b[0m next_tokens_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   2420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/logits_process.py:97\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/logits_process.py:333\u001b[0m, in \u001b[0;36mRepetitionPenaltyLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 333\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;66;03m# if score < 0 then repetition penalty has to be multiplied to reduce the token probabilities\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(score \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m, score \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty, score \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt_num = len(data_test)\n",
    "composed_resp = []\n",
    "for i in range(1):\n",
    "    model_original_resp = get_responses(prompt_num, data_test, context_accom)    \n",
    "    composed_resp.extend(model_original_resp)\n",
    "\n",
    "df_original_resp = pd.DataFrame(composed_resp, columns=['original response'])\n",
    "# df_original_resp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 10 th prompt responded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 20 th prompt responded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 30 th prompt responded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 40 th prompt responded\n"
     ]
    }
   ],
   "source": [
    "prompt_num = len(data_test)\n",
    "composed_resp = []\n",
    "for i in range(1):\n",
    "    model_original_resp = get_responses(prompt_num, data_test, context_full)    \n",
    "    composed_resp.extend(model_original_resp)\n",
    "\n",
    "df_original_resp2 = pd.DataFrame(composed_resp, columns=['original response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_labels(text):\n",
    "    # label patterns. Further wirk to extract label from certain key pairs only\n",
    "    pattern1 = r': \"(Policy\\sViolated|Policy\\sNot\\sViolated)\"'\n",
    "    pattern2 = r': £\\s*([\\d,]+\\.?\\d*)'  # extract the number after £ \n",
    "    pattern3 = r': \"(Fully\\s+Reimbursable|Partially\\s+Reimbursable|Not\\sReimbursable|Further\\sClarification\\sRequired)\"'\n",
    "    \n",
    "    # create match\n",
    "    match1 = re.search(pattern1, text)\n",
    "    match2 = re.search(pattern2, text)\n",
    "    match3 = re.search(pattern3, text)\n",
    "    \n",
    "    amount = 0\n",
    "    # processing Amount label, deleting comma\n",
    "    if match2:\n",
    "        try:\n",
    "            # Remove commas and convert to float\n",
    "            temp = match2.group(1)\n",
    "            amount = float(temp.replace(',', ''))\n",
    "        except ValueError:\n",
    "            amount = 0 \n",
    "\n",
    "    # find match\n",
    "    label1 = match1.group(1) if match1 else None\n",
    "    #label2 = amount.group(1) if match2 else \"0\"\n",
    "    label3 = match3.group(1) if match3 else None\n",
    "    # give a format marker , 1 if there's at least 1 classification label missing and 0 if not \n",
    "    label_format = 1 if label1 is None or label3 is None else 0 \n",
    "\n",
    "    return label1, amount, label3, label_format\n",
    "\n",
    "def extract_all_labels(scenario, response_list, num):\n",
    "    label_list = []\n",
    "    for i in range(num):\n",
    "        text = response_list['original response'][i]\n",
    "        label1, label2, label3,label_format = extract_labels(text)\n",
    "        label_list.append([scenario[i], text, label1, label2, label3,label_format])\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list = extract_all_labels(data_test['Prompt'], df_original_resp, prompt_num)\n",
    "\n",
    "df_label = pd.DataFrame(label_list, columns=['Scenario', 'Text', 'Classification T1', 'Reimbursable Amount', 'Classification T2','Format Compliance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def evaluate(response_df, prompt_df):\n",
    "    T1_accurate = 0\n",
    "    T2_accurate = 0\n",
    "    amount_accurate = 0\n",
    "    for i in range(len(response_df)):\n",
    "        pre_label = response_df[['Classification T1', 'Reimbursable Amount', 'Classification T2','Format Compliance']].iloc[i]\n",
    "        ori_label = prompt_df[['Classification T1', 'Amount', 'Classification T2']].iloc[i]\n",
    "\n",
    "        if pre_label['Classification T1'] == ori_label['Classification T1']:\n",
    "            T1_accurate += 1\n",
    "        if float(pre_label['Reimbursable Amount']) == float(ori_label['Amount']):\n",
    "            amount_accurate += 1\n",
    "        #print(f\"Predicted: {pre_label['Reimbursable Amount']}, should be {ori_label['Amount']}\")\n",
    "        if pre_label[ 'Classification T2'] == ori_label['Classification T2']:\n",
    "           T2_accurate += 1\n",
    "    T1_accuracy = T1_accurate/len(response_df) \n",
    "    T2_accuracy = T2_accurate/len(response_df)\n",
    "    amount_accuracy = amount_accurate/len(response_df)\n",
    "    format_accuracy = (1 - response_df['Format Compliance'].sum()/len(response_df)) * 100\n",
    "\n",
    "    # F1 score calculation\n",
    "    # To calculate F1 requires labelled inputs\n",
    "    encoder = LabelEncoder()\n",
    "    a = encoder.fit_transform(['Policy Violated', 'Policy Not Violated', 'None'])\n",
    "    f1_T1 = f1_score(encoder.transform(prompt_df['Classification T1'][:len(response_df)]), encoder.transform(response_df['Classification T1']), average='weighted')\n",
    "\n",
    "    print(a,encoder.transform(response_df['Classification T1']).sum())\n",
    "\n",
    "    encoder.fit(['Fully Reimbursable','Partially Reimbursable','Not Reimbursable','Further Clarification Required','None'])\n",
    "    #f1_amount = f1_score(prompt_df['Reimbursable Amount'], response_df['Reimbursement Amount'], average='weighted')\n",
    "    f1_T2 = f1_score(encoder.transform(prompt_df['Classification T2'][:len(response_df)]), encoder.transform(response_df['Classification T2']), average='weighted')\n",
    "\n",
    "    print(f\"T1 acc: {T1_accuracy}, T1 F1 Score: {f1_T1};\\nT2 acc: {T2_accuracy}, T2 weighted F1 Score: {f1_T2};\\namount acc: {amount_accuracy}, \\nformat acc: {format_accuracy} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0] 45\n",
      "T1 acc: 0.6, T1 F1 Score: 0.5826923076923076;\n",
      "T2 acc: 0.45, T2 weighted F1 Score: 0.4132006882006882;\n",
      "amount acc: 0.3, \n",
      "format acc: 95.0 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate(df_label, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now full policy:\n",
      "[2 1 0] 51\n",
      "T1 acc: 0.6, T1 F1 Score: 0.5786666666666667;\n",
      "T2 acc: 0.425, T2 weighted F1 Score: 0.37120708748615727;\n",
      "amount acc: 0.3, \n",
      "format acc: 100.0 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_list2 = extract_all_labels(data_test['Prompt'], df_original_resp2, prompt_num)\n",
    "\n",
    "df_label2 = pd.DataFrame(label_list2, columns=['Scenario', 'Text', 'Classification T1', 'Reimbursable Amount', 'Classification T2','Format Compliance'])\n",
    "print(\"now full policy:\")\n",
    "evaluate(df_label2, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# New Code\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "base_model = model_name\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        # load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Reduce VRAM use\n",
    "model.config.pretraining_tp = 1\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token\n",
    "\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2903])\n"
     ]
    }
   ],
   "source": [
    "p = dataset['test']['prompt'][0]\n",
    "input_po = tokenizer(p, return_tensors=\"pt\").to(\"cuda\")\n",
    "# input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(input_po['input_ids'].shape)\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceehuf/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 190/190 [00:04<00:00, 40.15 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 40.43 examples/s]\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=2e-5,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length= 3300,\n",
    "    max_prompt_length= 3072,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    warmup_steps=20,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results/\",\n",
    "\n",
    "    save_steps=20,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,  # Ensures the best model is loaded at the end\n",
    ")\n",
    "\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 3:12:33, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.579700</td>\n",
       "      <td>1.538651</td>\n",
       "      <td>9.210200</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.125215</td>\n",
       "      <td>-0.241181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115966</td>\n",
       "      <td>-2.411807</td>\n",
       "      <td>-1.252149</td>\n",
       "      <td>-3.154190</td>\n",
       "      <td>-3.153484</td>\n",
       "      <td>1.516210</td>\n",
       "      <td>-0.224416</td>\n",
       "      <td>1.403116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>0.384155</td>\n",
       "      <td>9.199100</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>-0.038132</td>\n",
       "      <td>-0.357635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319503</td>\n",
       "      <td>-3.576350</td>\n",
       "      <td>-0.381322</td>\n",
       "      <td>-2.796489</td>\n",
       "      <td>-2.795533</td>\n",
       "      <td>0.382410</td>\n",
       "      <td>-0.017449</td>\n",
       "      <td>4.488202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.039194</td>\n",
       "      <td>9.214500</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.011466</td>\n",
       "      <td>-0.425172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.413705</td>\n",
       "      <td>-4.251717</td>\n",
       "      <td>-0.114663</td>\n",
       "      <td>-2.175448</td>\n",
       "      <td>-2.173084</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>7.178544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.034273</td>\n",
       "      <td>9.202300</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.008884</td>\n",
       "      <td>-0.430047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421163</td>\n",
       "      <td>-4.300467</td>\n",
       "      <td>-0.088840</td>\n",
       "      <td>-2.507337</td>\n",
       "      <td>-2.505198</td>\n",
       "      <td>0.034095</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>7.396731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>9.210700</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.008177</td>\n",
       "      <td>-0.443117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434940</td>\n",
       "      <td>-4.431169</td>\n",
       "      <td>-0.081767</td>\n",
       "      <td>-2.581347</td>\n",
       "      <td>-2.579115</td>\n",
       "      <td>0.032182</td>\n",
       "      <td>-0.001434</td>\n",
       "      <td>7.780704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.030870</td>\n",
       "      <td>9.205700</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.008079</td>\n",
       "      <td>-0.455634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.447555</td>\n",
       "      <td>-4.556339</td>\n",
       "      <td>-0.080792</td>\n",
       "      <td>-2.587422</td>\n",
       "      <td>-2.585092</td>\n",
       "      <td>0.030743</td>\n",
       "      <td>-0.001268</td>\n",
       "      <td>8.068693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.029347</td>\n",
       "      <td>9.206600</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.008014</td>\n",
       "      <td>-0.462234</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454220</td>\n",
       "      <td>-4.622344</td>\n",
       "      <td>-0.080142</td>\n",
       "      <td>-2.581806</td>\n",
       "      <td>-2.579388</td>\n",
       "      <td>0.029227</td>\n",
       "      <td>-0.001205</td>\n",
       "      <td>8.299503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.027884</td>\n",
       "      <td>9.210900</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.007796</td>\n",
       "      <td>-0.466496</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>-4.664957</td>\n",
       "      <td>-0.077960</td>\n",
       "      <td>-2.557693</td>\n",
       "      <td>-2.555197</td>\n",
       "      <td>0.027773</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>8.348478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.026634</td>\n",
       "      <td>9.200200</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.007820</td>\n",
       "      <td>-0.469638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461817</td>\n",
       "      <td>-4.696378</td>\n",
       "      <td>-0.078205</td>\n",
       "      <td>-2.551430</td>\n",
       "      <td>-2.548878</td>\n",
       "      <td>0.026525</td>\n",
       "      <td>-0.001094</td>\n",
       "      <td>8.434547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.026039</td>\n",
       "      <td>9.200800</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.007780</td>\n",
       "      <td>-0.470929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.463149</td>\n",
       "      <td>-4.709290</td>\n",
       "      <td>-0.077804</td>\n",
       "      <td>-2.542892</td>\n",
       "      <td>-2.540333</td>\n",
       "      <td>0.025932</td>\n",
       "      <td>-0.001067</td>\n",
       "      <td>8.454558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.025921</td>\n",
       "      <td>9.206400</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.007737</td>\n",
       "      <td>-0.471940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.464203</td>\n",
       "      <td>-4.719398</td>\n",
       "      <td>-0.077372</td>\n",
       "      <td>-2.542535</td>\n",
       "      <td>-2.539967</td>\n",
       "      <td>0.025815</td>\n",
       "      <td>-0.001057</td>\n",
       "      <td>8.491469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.025906</td>\n",
       "      <td>9.210800</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>-0.007753</td>\n",
       "      <td>-0.471787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.464034</td>\n",
       "      <td>-4.717872</td>\n",
       "      <td>-0.077529</td>\n",
       "      <td>-2.541809</td>\n",
       "      <td>-2.539250</td>\n",
       "      <td>0.025801</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>8.484040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66957041-2b449c895f5429580e1b24a3;da830b73-3931-48aa-b3ec-6263cf1c8e19)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66957404-598e80c04c7543390949ff63;0b383e13-13e5-4dcb-9777-0c8f84df9555)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-669577c8-4689c3b424b694412e65db6e;62dcba46-c571-4482-b360-814bdaf01117)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66957b93-4821243407f9c8e561f7f8fa;d5c6d402-d438-4aca-996c-fa6b49c509be)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66957f57-3b5ee10d1ab3bd7772f48658;fb123f9a-b2b6-48eb-a076-c82856a9944d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6695831a-527f9f7318d444ab1f7b1bbe;627e672a-f556-4a98-bab4-0f9a57d06788)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-140 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-669586e4-6ff58a4b26d16a7d7d373303;15e48ca5-a886-4dc9-9321-8d9d51163ded)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-160 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66958aad-63e1a5b350fa9a5614b0c27a;1eb5dd1e-8713-4698-9ba0-c4ebbd052f3d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-180 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66958e6f-5640a5e13aec844055611ac6;0e78dbaa-0bf9-4578-ad0d-19a1acddcf23)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66959238-2527255332b04a927876adab;d9403a08-3fb1-4d78-8dc2-2b764703c038)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-669595fd-7219ce817cbd993f6882b507;1b9079ca-aae7-4a8a-902c-9fb5e9fc3673)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-669599c5-6f421cb50b5521ee09c0c202;08d20016-60a3-4a3b-bd94-c871dd31411f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-669599c7-30115bf96355fd91443fe27f;b82cce11-0989-4e96-af41-da6af79c4485)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_model = 'ORPO-Mistral-7b'\n",
    "orpo_trainer.train()\n",
    "orpo_trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code NANs happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False, #True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        # load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Reduce VRAM use\n",
    "model.config.pretraining_tp = 1\n",
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceehuf/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:209: UserWarning: `max_length` is not set in the ORPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 190/190 [00:03<00:00, 47.73 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 46.98 examples/s]\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=2e-5,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    #max_length= ,\n",
    "    max_prompt_length=5120,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=20,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results/\",\n",
    ")\n",
    "\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunyu-zhu-23\u001b[0m (\u001b[33mucl_student\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/uceehuf/msc_const_ai-2/notebooks/wandb/run-20240711_193723-dic4o91b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ucl_student/huggingface/runs/dic4o91b' target=\"_blank\">swift-field-48</a></strong> to <a href='https://wandb.ai/ucl_student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ucl_student/huggingface' target=\"_blank\">https://wandb.ai/ucl_student/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ucl_student/huggingface/runs/dic4o91b' target=\"_blank\">https://wandb.ai/ucl_student/huggingface/runs/dic4o91b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceehuf/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/94 11:52 < 1:37:24, 0.01 it/s, Epoch 0.23/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>49.818700</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>-3.133734</td>\n",
       "      <td>-3.133734</td>\n",
       "      <td>2.068200</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>49.846200</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>-3.133734</td>\n",
       "      <td>-3.133734</td>\n",
       "      <td>2.068200</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43morpo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/accelerator.py:2134\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2134\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "orpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Flush memory\n",
    "del orpo_trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceehuf/.local/lib/python3.11/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-669599c7-581eec9118dbcbc51117cb84;9a1f2ad8-343b-4cad-b1c3-2b02e027c535)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "new_model = 'ORPO-Mistral-7b'\n",
    "orpo_trainer.save_model(new_model)\n",
    "\n",
    "base_model = model_name\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"/scratch/uceehuf/Mistral_orpo_acc1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Check if paths exist\n",
    "assert os.path.exists(output_dir), f\"Output directory {output_dir} does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/scratch/uceehuf/Mistral_orpo_acc1/tokenizer_config.json',\n",
       " '/scratch/uceehuf/Mistral_orpo_acc1/special_tokens_map.json',\n",
       " '/scratch/uceehuf/Mistral_orpo_acc1/tokenizer.json')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTed Model at path: /scratch/uceehuf/Mistral_orpo_acc1 loaded\n"
     ]
    }
   ],
   "source": [
    "#load fted model\n",
    "new_model_path = output_dir\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(new_model_path,\n",
    "    trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "print(f\"FTed Model at path: {new_model_path} loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m composed_resp \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     model_original_resp \u001b[38;5;241m=\u001b[39m \u001b[43mget_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_full\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      5\u001b[0m     composed_resp\u001b[38;5;241m.\u001b[39mextend(model_original_resp)\n\u001b[1;32m      7\u001b[0m df_fted_resp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(composed_resp, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal response\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[9], line 71\u001b[0m, in \u001b[0;36mget_responses\u001b[0;34m(prompt_num, data_frame, context)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# print(\"tokenization completed\",input_ids['input_ids'].shape)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#get \"Answer\" token ID in the tokenizer, forcing the model to start answering with \"Answer\" and thus give a json output\u001b[39;00m\n\u001b[1;32m     69\u001b[0m start_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m#do_sample = False, temperature = 5, top_k = 0.9, \u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_token_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# alter max number of tokens here\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# trim the output by removing prompt\u001b[39;00m\n\u001b[1;32m     77\u001b[0m model_response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2408\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1033\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1034\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         use_cache,\n\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:757\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:654\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    651\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    653\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m--> 654\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    657\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt_num = len(data_test)\n",
    "composed_resp = []\n",
    "for i in range(1):\n",
    "    model_original_resp = get_responses(prompt_num, data_test, context_full)    \n",
    "    composed_resp.extend(model_original_resp)\n",
    "\n",
    "df_fted_resp = pd.DataFrame(composed_resp, columns=['original response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fine-tuned:\n",
      "[2 1 0] 25\n",
      "T1 acc: 0.3, T1 F1 Score: 0.3916666666666666;\n",
      "T2 acc: 0.025, T2 weighted F1 Score: 0.025;\n",
      "amount acc: 0.4, \n",
      "format acc: 15.000000000000002 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_list_ft = extract_all_labels(data_test['Prompt'], df_fted_resp, prompt_num)\n",
    "\n",
    "df_label_ft = pd.DataFrame(label_list_ft, columns=['Scenario', 'Text', 'Classification T1', 'Reimbursable Amount', 'Classification T2','Format Compliance'])\n",
    "print(\"Now fine-tuned:\")\n",
    "evaluate(df_label_ft, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 10 th prompt responded\n",
      "the 20 th prompt responded\n",
      "the 30 th prompt responded\n",
      "the 40 th prompt responded\n"
     ]
    }
   ],
   "source": [
    "prompt_num = len(data_test)\n",
    "composed_resp = []\n",
    "for i in range(1):\n",
    "    model_original_resp = get_responses(prompt_num, data_test, context_accom)    \n",
    "    composed_resp.extend(model_original_resp)\n",
    "\n",
    "df_fted_resp2 = pd.DataFrame(composed_resp, columns=['original response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fine-tuned, only acc policy:\n",
      "[2 1 0] 23\n",
      "T1 acc: 0.25, T1 F1 Score: 0.36342592592592593;\n",
      "T2 acc: 0.15, T2 weighted F1 Score: 0.21718227424749165;\n",
      "amount acc: 0.4, \n",
      "format acc: 30.000000000000004 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_list_ft2 = extract_all_labels(data_test['Prompt'], df_fted_resp2, prompt_num)\n",
    "\n",
    "df_label_ft2 = pd.DataFrame(label_list_ft2, columns=['Scenario', 'Text', 'Classification T1', 'Reimbursable Amount', 'Classification T2','Format Compliance'])\n",
    "\n",
    "print(\"Now fine-tuned, only acc policy:\")\n",
    "evaluate(df_label_ft2, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer, max_prompt_length, *args, **kwargs):\n",
    "        super().__init__(tokenizer, *args, **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_prompt_length = max_prompt_length\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Tokenize and truncate the prompt, chosen, and rejected\n",
    "        tokenized_inputs = {\n",
    "            'input_ids_prompt': [],\n",
    "            'input_ids_chosen': [],\n",
    "            'input_ids_rejected': [],\n",
    "            'attention_mask_prompt': [],\n",
    "            'attention_mask_chosen': [],\n",
    "            'attention_mask_rejected': []\n",
    "        }\n",
    "\n",
    "        for feature in features:\n",
    "            prompt = feature[\"prompt\"]\n",
    "            chosen = feature[\"chosen\"]\n",
    "            rejected = feature[\"rejected\"]\n",
    "            \n",
    "            # Tokenize and truncate\n",
    "            prompt_tokens = self.tokenizer(prompt, truncation=True, max_length=self.max_prompt_length, padding='max_length')\n",
    "            chosen_tokens = self.tokenizer(chosen, truncation=True, max_length=self.max_prompt_length, padding='max_length')\n",
    "            rejected_tokens = self.tokenizer(rejected, truncation=True, max_length=self.max_prompt_length, padding='max_length')\n",
    "\n",
    "            tokenized_inputs['input_ids_prompt'].append(prompt_tokens[\"input_ids\"])\n",
    "            tokenized_inputs['input_ids_chosen'].append(chosen_tokens[\"input_ids\"])\n",
    "            tokenized_inputs['input_ids_rejected'].append(rejected_tokens[\"input_ids\"])\n",
    "            tokenized_inputs['attention_mask_prompt'].append(prompt_tokens[\"attention_mask\"])\n",
    "            tokenized_inputs['attention_mask_chosen'].append(chosen_tokens[\"attention_mask\"])\n",
    "            tokenized_inputs['attention_mask_rejected'].append(rejected_tokens[\"attention_mask\"])\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        for key in tokenized_inputs:\n",
    "            tokenized_inputs[key] = torch.tensor(tokenized_inputs[key])\n",
    "\n",
    "        return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  Prompt Tokens - ['<s>', '▁A', '▁<<', '<', 'POL', 'IC', 'Y', '>>>', '▁and', '▁a', '▁<<', '<', 'SC', 'EN', 'AR', 'IO', '>>>', '▁will', '▁be', '▁provided', ',', '▁you', '▁are', '▁G', 'RE', 'AT', '▁at', '▁analys', 'ing', '▁the', '▁<<', '<', 'SC', 'EN', 'AR', 'IO', '>>>', '▁according', '▁to', '▁the', '▁<<', '<', 'POL', 'IC', 'Y', '>>', '>.', '▁Your', '▁answer', '▁M', 'UST', '▁and', '▁and', '▁CAN', '▁ON', 'LY', '▁be', '▁a', '▁valid', '▁json', '▁format', ',', '▁having', '▁', '4', '▁text', '▁fields', ':', '▁Class', 'ification', '▁T', '1', ',', '▁Re', 'im', 'bur', 'se', 'ment', '▁Am', 'ount', ',', '▁Class', 'ification', '▁T', '2', ',', '▁and', '▁Re', 'asons', '.', '▁Your', '▁answer', '▁should', '▁follow', '▁these', '▁structures', ':', '▁', '<0x0A>', '▁', '1', ')', '▁\"', 'Class', 'ification', '▁T', '1', '\"', '▁:', 'you', '▁M', 'UST', '▁choose', '▁the', '▁activity', '▁in', '▁the', '▁<<', '<', 'sc', 'enario', '>>>', '▁is', \"▁'\", 'Policy', '▁Vi', 'ol', 'ated', \"'\", '▁or', \"▁'\", 'Policy', '▁Not', '▁Vi', 'ol', 'ated', \"'\", '<0x0A>', '▁', '<0x0A>', '▁', '2', ')', '▁\"', 'Re', 'im', 'bur', 'se', 'ment', '▁Am', 'ount', '\"', '▁:', '▁Answer', '▁according', '▁to', '▁the', '▁Policy', '▁the', '▁amount', '▁of', '▁money', '▁can', '▁be', '▁re', 'imb', 'urs', 'ed', '▁totally', '▁', '<0x0A>', '▁', '3', ')', '▁\"', 'Class', 'ification', '▁T', '2', '\"', '▁:', '▁Comp', 'are', '▁to', '▁the', '▁requested', '▁re', 'im', 'bur', 'se', 'ment', '▁amount', '▁in', '▁the', '▁scenario', ',', '▁and', '▁answer', '▁if', '▁the', '▁claim', '▁is', \"▁'\", 'F', 'ully', '▁Re', 'imb', 'urs', 'able', \"'\", \"/'\", 'Part', 'ially', '▁Re', 'imb', 'urs', 'able', \"'\", \"/'\", 'Not', '▁Re', 'imb', 'urs', 'able', \"'\", \"/'\", 'F', 'ur', 'ther', '▁Clar', 'ification', '▁Required', \"'.\", '▁', '<0x0A>', '▁', '4', ')', '▁give', '▁reasons', '▁for', '▁your', '▁choices', '▁according', '▁to', '▁the', '▁<<', '<', 'POL', 'IC', 'Y', '>>', '>.', '<0x0A>', '▁▁▁', '▁<<', '<<', '<', 'Sc', 'enario', '▁Start', ':', '<0x0A>', '<0x0A>', 'I', '▁have', '▁to', '▁travel', '▁to', '▁Berlin', '▁for', '▁a', '▁research', '▁meeting', ',', '▁staying', '▁at', '▁a', '▁hotel', '▁for', '▁', '4', '▁nights', '▁at', '▁a', '▁total', '▁cost', '▁of', '▁£', '7', '0', '0', '.', '▁Can', '▁this', '▁be', '▁exp', 'ensed', '?', '<0x0A>', '<0x0A>', 'Sc', 'enario', '▁End', '>>', '>>', '><', '<<<<', 'Policy', '▁Start', ':', '<0x0A>', '<0x0A>', 'G', 'NE', 'I', '▁Exp', 'enses', '▁Policy', '<0x0A>', 'Summary', '▁Gu', 'id', 'ance', '<0x0A>', 'Subject', '<0x0A>', 'Page', '▁No', '.', '<0x0A>', 'Tr', 'ain', '▁Travel', '<0x0A>', 'This', '▁should', '▁be', '▁standard', '▁class', '▁unless', '▁a', '▁heavily', '▁discount', 'ed', '▁first', '-', 'class', '▁ticket', '▁is', '▁book', 'ed', '▁significantly', '▁in', '▁advance', '▁of', '▁the', '▁date', '▁of', '▁travel', '▁via', '▁G', 'NE', 'I', \"'\", 's', '▁approved', '▁travel', '▁provider', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'T', 'axis', '<0x0A>', 'T', 'axis', '▁may', '▁be', '▁used', '▁where', '▁there', '▁is', '▁a', '▁clear', '▁requirement', '▁or', '▁where', '▁they', '▁are', '▁the', '▁most', '▁economic', '▁practical', '▁means', '▁of', '▁transport', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Bus', 'iness', '▁M', 'ile', 'age', '<0x0A>', 'C', 'ars', '▁-', '▁', '5', '0', 'p', '▁per', '▁mile', '▁for', '▁the', '▁first', '▁', '1', '1', ',', '0', '0', '0', '▁miles', '▁in', '▁a', '▁tax', '▁year', '▁and', '▁', '2', '8', 'p', '▁per', '▁mile', '▁there', 'after', ',', '▁Motor', 'cy', 'cles', '▁-', '▁', '2', '6', 'p', '▁per', '▁mile', ',', '▁B', 'icy', 'cles', '▁-', '▁', '2', '2', 'p', '▁per', '▁mile', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Air', '▁Travel', '<0x0A>', 'This', '▁should', '▁be', '▁book', 'ed', '▁using', '▁G', 'NE', 'I', '’', 's', '▁preferred', '▁travel', '▁provider', '▁where', '▁possible', '.', '▁For', '▁flights', '▁of', '▁five', '▁hours', '▁or', '▁less', ',', '▁staff', '▁should', '▁book', '▁restricted', '▁tickets', '▁in', '▁economy', '.', '▁For', '▁flights', '▁lasting', '▁five', '▁hours', '▁or', '▁longer', '▁staff', '▁may', '▁travel', '▁premium', '▁economy', '.', '▁Where', '▁a', '▁long', '▁international', '▁flight', '▁is', '▁immediately', '▁followed', '▁by', '▁a', '▁presentation', '▁or', '▁meeting', ',', '▁staff', '▁may', '▁travel', '▁by', '▁business', '▁class', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Hot', 'el', '▁R', 'ates', '<0x0A>', 'UK', ':', '▁should', '▁not', '▁exceed', '▁£', '2', '1', '0', '▁including', '▁V', 'AT', '▁(', 'room', '▁only', ')', '▁in', '▁London', '▁or', '▁£', '1', '4', '0', '▁including', '▁V', 'AT', '▁(', 'room', '▁only', ')', '▁outside', '▁of', '▁London', '.', '▁The', '▁cost', '▁of', '▁accommodation', '▁at', '▁or', '▁near', '▁the', '▁normal', '▁place', '▁of', '▁work', '▁wouldn', '’', 't', '▁qualify', '▁as', '▁business', '▁travel', '▁and', '▁would', '▁therefore', '▁be', '▁subject', '▁to', '▁tax', '.', '<0x0A>', 'Over', 'se', 'as', ':', '▁not', '▁subject', '▁to', '▁a', '▁fixed', '▁rate', '▁or', '▁limit', '.', '▁Cl', 'aim', 'ants', '▁should', '▁instead', '▁look', '▁to', '▁apply', '▁this', '▁policy', \"'\", 's', '▁general', '▁principles', '▁(', 'section', '▁', '3', ')', '▁when', '▁assess', 'ing', '▁reason', 'ab', 'len', 'ess', '▁and', '▁use', '▁either', '▁H', 'M', 'RC', \"'\", 's', '▁overseas', '▁bench', 'mark', '▁rates', '▁and', '▁/', '▁or', '▁this', '▁policy', \"'\", 's', '▁equivalent', '▁UK', '▁rates', '▁for', '▁guidance', '<0x0A>', '<0x0A>', '<0x0A>', 'Me', 'als', '▁whilst', '▁away', '▁overnight', '<0x0A>', 'UK', '▁Break', 'fast', ':', '▁maximum', '▁of', '▁£', '1', '2', '▁rece', 'ipt', 'ed', '▁including', '▁V', 'AT', '▁and', '▁service', ';', '▁unre', 'ce', 'ipt', 'ed', '▁£', '5', '.', '5', '<0x0A>', 'UK', '▁L', 'unch', ':', '▁maximum', '▁of', '▁£', '1', '8', '▁rece', 'ipt', 'ed', '▁including', '▁V', 'AT', '▁and', '▁service', ';', '▁unre', 'ce', 'ipt', 'ed', '▁£', '6', '<0x0A>', 'UK', '▁D', 'inner', ':', '▁maximum', '▁of', '▁£', '3', '6', '▁rece', 'ipt', 'ed', '▁including', '▁V', 'AT', '▁and', '▁service', ',', '▁unre', 'ce', 'ipt', 'ed', '▁£', '1', '7', '.', '<0x0A>', 'UK', '▁', '2', '4', '-', 'hour', '▁rate', ':', '▁In', '▁line', '▁with', '▁the', '▁above', ',', '▁up', '▁to', '▁£', '6', '6', '▁rece', 'ipt', 'ed', '▁(', '£', '2', '8', '▁unre', 'ce', 'ipt', 'ed', ').', '<0x0A>', 'Over', 'se', 'as', ':', '▁not', '▁subject', '▁to', '▁a', '▁fixed', '▁rate', '▁or', '▁limit', '.', '▁Cl', 'aim', 'ants', '▁should', '▁instead', '▁look', '▁to', '▁apply', '▁this', '▁policy', \"'\", 's', '▁general', '▁principles', '▁(', 'section', '▁', '3', ')', '▁when', '▁assess', 'ing', '▁reason', 'ab', 'len', 'ess', '▁and', '▁use', '▁either', '▁H', 'M', 'RC', \"'\", 's', '▁overseas', '▁bench', 'mark', '▁rates', '▁and', '▁/', '▁or', '▁this', '▁policy', \"'\", 's', '▁equivalent', '▁UK', '▁rates', '▁for', '▁guidance', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Line', '▁R', 'ental', '<0x0A>', 'Line', '▁rental', '▁cannot', '▁be', '▁claimed', ';', '▁only', '▁the', '▁cost', '▁of', '▁business', '▁calls', '▁on', '▁a', '▁call', '-', 'by', '-', 'call', '▁basis', '▁can', '▁be', '▁claimed', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Broad', 'band', '<0x0A>', 'Home', '▁internet', '▁connection', '▁cannot', '▁be', '▁claimed', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'St', 'aff', '▁and', '▁Student', '▁Enter', 't', 'aining', '<0x0A>', 'This', '▁means', '▁food', '▁or', '▁drink', '▁for', '▁two', '▁or', '▁more', '▁members', '▁of', '▁staff', '▁or', '▁registered', '▁G', 'NE', 'I', '▁students', '▁in', '▁connection', '▁with', '▁G', 'NE', 'I', '▁business', '▁activities', '.', '▁The', '▁cost', '▁of', '▁entertaining', '▁should', '▁not', '▁exceed', '▁£', '2', '2', '▁per', '▁head', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'St', 'aff', '▁Social', '▁Fun', 'ctions', '<0x0A>', 'Events', '▁that', '▁are', '▁not', '▁in', '▁connection', '▁to', '▁G', 'NE', 'I', '▁business', '▁activities', '▁e', '.', 'g', '.', '▁Christmas', '▁lun', 'ches', '/', 'part', 'ies', ',', '▁end', '▁of', '▁term', '▁social', 's', ',', '▁retirement', '▁parties', '▁etc', '.', '▁The', '▁G', 'NE', 'I', '▁department', '▁contribution', '▁should', '▁be', '▁no', '▁more', '▁than', '▁£', '2', '2', '▁per', '▁head', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Bus', 'iness', '▁entertaining', '<0x0A>', 'Cost', '▁should', '▁be', '▁appropriate', '▁and', '▁not', '▁exceed', '▁£', '4', '4', '▁per', '▁head', '▁including', '▁alcohol', '▁and', '▁service', '▁unless', '▁exceptional', '▁circumstances', '▁apply', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Pro', 'f', 'ess', 'ional', '▁Sub', 'scriptions', '<0x0A>', 'G', 'NE', 'I', '▁will', '▁only', '▁re', 'im', 'bur', 'se', '▁staff', ',', '▁or', '▁pay', '▁on', '▁their', '▁behalf', ',', '▁annual', '▁sub', 'scriptions', '▁or', '▁membership', 's', '▁to', '▁a', '▁professional', '▁body', '▁where', '▁either', ':', '<0x0A>', 'S', 'av', 'ings', '▁to', '▁G', 'NE', 'I', '▁ar', 'ising', '▁from', '▁membership', ',', '▁for', '▁example', '▁reduced', '▁conference', '▁attendance', '▁fees', ',', '▁exceed', '▁the', '▁cost', '▁of', '▁membership', ',', '▁or', '<0x0A>', 'Members', 'hip', '▁is', '▁mandatory', '▁in', '▁order', '▁to', '▁be', '▁able', '▁to', '▁teach', '▁on', '▁a', '▁profession', 'ally', '▁acc', 'red', 'ited', '▁course', '.', '<0x0A>', 'In', '▁all', '▁cases', ',', '▁the', '▁professional', '▁body', '▁must', '▁feature', '▁in', '▁the', '▁list', '▁of', '▁approved', '▁organisations', '▁published', '▁by', '▁H', 'M', 'RC', '.', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '1', '.', '▁Introduction', '▁', '<0x0A>', 'This', '▁Exp', 'enses', '▁Policy', '▁(', '‘', 'the', '▁Policy', '’', ')', '▁provides', '▁guidance', '▁to', '▁all', '▁individuals', '▁claiming', '▁re', 'im', 'bur', 'se', 'ment', '▁of', '▁reasonable', '▁expenses', '▁in', 'cur', 'red', '▁in', '▁connection', '▁with', '▁G', 'NE', 'I', '▁business', '.', '<0x0A>', 'G', 'NE', 'I', '▁is', '▁a', '▁charity', '▁and', '▁a', '▁large', '▁recipient', '▁of', '▁public', '▁funds', '▁from', '▁UK', 'RI', '▁and', '▁the', '▁Office', '▁for', '▁Students', ',', '▁grants', '▁from', '▁other', '▁public', '▁bodies', ',', '▁char', 'ities', '▁and', '▁fees', '▁paid', '▁by', '▁students', '.', '▁All', '▁expend', 'iture', '▁should', '▁be', '▁appropriate', '▁and', '▁modest', '▁in', '▁scale', '.', '<0x0A>', 'This', '▁policy', '▁has', '▁been', '▁prepared', '▁in', '▁accordance', '▁with', '▁In', 'come', '▁Tax', '▁and', '▁National', '▁Insurance', '▁Contribut', 'ion', '▁regulations', '▁and', '▁Her', '▁Maj', 'esty', '’', 's', '▁Rev', 'enue', '▁and', '▁Custom', 's', '▁(', 'H', 'M', 'RC', ')', '▁regulations', '.', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '2', '.', '▁S', 'cope', '<0x0A>', 'This', '▁policy', '▁applies', '▁to', '▁all', '▁spend', '▁on', '▁G', 'NE', 'I', '▁activities', '▁including', '▁from', '▁research', '▁grants', '▁and', '▁discret', 'ion', 'ary', '▁accounts', '▁as', '▁well', '▁as', '▁department', 'al', '▁codes', '.', '▁', '<0x0A>', 'This', '▁policy', '▁covers', '▁the', '▁following', '▁areas', '▁of', '▁expend', 'iture', ':', '<0x0A>', 'Tr', 'avel', '<0x0A>', 'Over', 'night', '▁costs', '▁and', '▁allow', 'ances', '<0x0A>', 'Tele', 'phone', '▁and', '▁internet', '▁costs', '<0x0A>', 'Enter', 'tainment', '▁&', '▁Hospital', 'ity', '<0x0A>', 'Tr', 'aining', '<0x0A>', 'Sub', 'scriptions', '<0x0A>', 'Adv', 'ances', '<0x0A>', 'Other', '▁Ex', 'pend', 'iture', '<0x0A>', 'For', '▁items', '▁that', '▁do', '▁not', '▁fall', '▁under', '▁any', '▁of', '▁the', '▁above', '▁head', 'ings', ',', '▁please', '▁refer', '▁to', '▁your', '▁local', '▁finance', '▁team', '▁or', '▁Account', 's', '▁Pay', 'able', '▁at', '▁expenses', '@', 'g', 'ne', 'i', '.', 'ac', '.', 'uk', '.', '<0x0A>', 'For', '▁expense', '▁claim', '▁procedures', ',', '▁please', '▁refer', '▁to', '▁Anne', 'x', '▁', '1', '.', '<0x0A>', 'Gener', 'ally', ',', '▁G', 'NE', 'I', '▁will', '▁not', '▁re', 'im', 'bur', 'se', '▁individuals', '▁for', ':', '<0x0A>', 'Broad', 'band', '/', 'Intern', 'et', '▁Prov', 'ision', '<0x0A>', 'Equ', 'ip', 'ment', '<0x0A>', 'F', 'ines', '<0x0A>', 'G', 'ifts', '<0x0A>', 'Ins', 'urance', '<0x0A>', 'Mobile', '▁Phone', '▁Contract', 's', '/', 'Hard', 'ware', '<0x0A>', 'Person', 'al', '▁Ex', 'pend', 'iture', '<0x0A>', 'Pro', 'f', 'ess', 'ional', '▁Sub', 'scriptions', '<0x0A>', 'Station', 'ery', '<0x0A>', 'For', '▁further', '▁details', '▁on', '▁the', '▁above', '▁excl', 'usions', ',', '▁please', '▁refer', '▁to', '▁Anne', 'x', '▁', '2', '.', '<0x0A>', 'Information', '▁on', '▁V', 'AT', '▁aspects', '▁which', '▁are', '▁common', '▁to', '▁all', '▁claims', '▁can', '▁be', '▁found', '▁at', '▁Anne', 'x', '▁', '3', '.', '<0x0A>', 'This', '▁policy', '▁also', '▁provides', '▁information', '▁on', '▁subs', 'istence', '▁payments', '▁for', '▁visiting', '▁Research', '▁F', 'ell', 'ows', ',', '▁please', '▁refer', '▁to', '▁Anne', 'x', '▁', '4', '.', '<0x0A>', 'Child', 'care', '▁and', '▁caring', '▁responsibility', '▁can', '▁be', '▁claimed', '▁in', '▁certain', '▁circumstances', ',', '▁please', '▁refer', '▁to', '▁Anne', 'x', '▁', '5', '.', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '3', '.', '▁General', '▁Prin', 'ciples', '<0x0A>', 'G', 'NE', 'I', '▁employees', '▁and', '▁all', '▁others', '▁engaged', '▁in', '▁G', 'NE', 'I', '▁activity', '▁(', 'collect', 'ively', '▁claim', 'ants', ')', '▁will', '▁be', '▁re', 'imb', 'urs', 'ed', '▁for', '▁the', '▁actual', '▁cost', '▁of', '▁expenses', '▁in', 'cur', 'red', ',', '▁wh', 'olly', ',', '▁exclusively', '▁and', '▁necessarily', '▁in', '▁the', '▁performance', '▁of', '▁G', 'NE', 'I', '▁activity', '▁as', '▁presc', 'ribed', '▁in', '▁this', '▁policy', '.', '<0x0A>', 'The', '▁expense', '▁must', '▁be', '▁just', 'ifiable', '▁and', '▁reasonable', '▁according', '▁to', '▁the', '▁information', '▁in', '▁this', '▁policy', ',', '▁and', '▁the', '▁claim', '▁should', '▁always', '▁be', '▁prepared', '▁honestly', ',', '▁legally', '▁and', '▁respons', 'ibly', '.', '▁Any', '▁bre', 'ach', '▁of', '▁the', '▁policy', '▁could', '▁lead', '▁to', '▁discipl', 'inary', '▁action', ',', '▁up', '▁to', '▁and', '▁including', '▁dismiss', 'al', '.', '<0x0A>', 'Wh', 'ilst', '▁this', '▁policy', '▁aims', '▁to', '▁provide', '▁comprehensive', '▁guidance', '▁on', '▁re', 'imb', 'urs', 'able', '▁out', '-', 'of', '-', '▁pocket', '▁expend', 'iture', ',', '▁it', '▁is', '▁recommended', '▁that', '▁staff', '▁still', '▁seek', '▁advance', '▁approval', '▁from', '▁their', '▁budget', '▁holder', '▁or', '▁department', '▁manager', '▁in', '▁any', '▁situation', '▁where', '▁interpretation', '▁of', '▁this', '▁policy', '▁is', '▁in', '▁doubt', '.', '<0x0A>', 'Em', 'ploy', 'ees', '▁should', '▁be', '▁able', '▁to', '▁proc', 'ure', '▁the', '▁majority', '▁of', '▁goods', '▁and', '▁services', '▁required', '▁for', '▁G', 'NE', 'I', '▁business', '▁using', '▁the', '▁standard', '▁purchase', '▁to', '▁payment', '▁procedures', '▁using', '▁the', '▁G', 'NE', 'IF', 'in', 'ance', '▁system', '.', '<0x0A>', 'It', '▁is', '▁the', '▁responsibility', '▁of', '▁author', 'ised', '▁sign', 'ator', 'ies', '▁to', '▁ensure', '▁that', '▁this', '▁expenses', '▁policy', '▁is', '▁up', 'held', ';', '▁and', '▁it', '▁should', '▁be', '▁noted', '▁that', '▁individual', '▁claims', '▁falling', '▁outside', '▁of', '▁the', '▁policy', '▁may', '▁be', '▁subject', '▁to', '▁review', '▁and', '/', 'or', '▁re', 'jection', '▁by', '▁Finance', '.', '<0x0A>', 'E', 'lect', 'ronic', '▁images', '▁of', '▁original', '▁rece', 'i', 'pts', '▁must', '▁accompany', '▁all', '▁claims', '.', '▁Credit', '▁card', '▁sl', 'ips', '▁or', '▁credit', '▁card', '/', 'bank', '▁statements', '▁will', '▁not', '▁be', '▁accepted', '▁as', '▁evidence', '▁of', '▁business', '▁expend', 'iture', '.', '▁All', '▁rece', 'i', 'pts', '▁used', '▁should', '▁include', '▁details', '▁of', '▁what', '▁goods', '▁or', '▁services', '▁have', '▁been', '▁purchased', '.', '▁Original', '▁paper', '▁rece', 'i', 'pts', '▁do', '▁not', '▁need', '▁to', '▁be', '▁stored', '▁locally', ',', '▁unless', '▁they', '▁relate', '▁to', '▁sponsored', '▁research', '▁and', '▁it', '▁is', '▁explicit', '▁in', '▁the', '▁terms', '▁of', '▁the', '▁grant', '▁that', '▁original', '▁paper', '▁rece', 'i', 'pts', '▁need', '▁to', '▁be', '▁retained', '.', '<0x0A>', 'When', '▁making', '▁a', '▁claim', '▁for', '▁a', '▁group', ',', '▁the', '▁most', '▁senior', '▁person', '▁present', '▁should', '▁pay', '▁for', '▁the', '▁expend', 'iture', '▁and', '▁make', '▁the', '▁claim', '.', '<0x0A>', 'Exp', 'enses', '▁should', '▁be', '▁submitted', '▁as', '▁soon', '▁as', '▁possible', '▁after', '▁they', '▁have', '▁been', '▁in', 'cur', 'red', '.', '▁Cl', 'aim', 's', '▁must', '▁be', '▁made', '▁within', '▁three', '▁months', '.', '▁Appro', 'val', '▁should', '▁be', '▁gained', '▁from', '▁the', '▁relevant', '▁School', '▁or', '▁PS', '▁Head', '▁of', '▁Finance', '▁if', '▁the', '▁claim', '▁is', '▁over', '▁three', '▁months', '▁old', '.', '<0x0A>', 'It', '▁costs', '▁a', '▁fixed', '▁amount', '▁to', '▁process', '▁any', '▁claim', ',', '▁so', '▁claim', 'ants', '▁should', '▁ende', 'av', 'our', '▁not', '▁to', '▁submit', '▁claims', '▁for', '▁less', '▁than', '▁£', '3', '3', ',', '▁unless', '▁they', '▁represent', '▁the', '▁total', '▁of', '▁expenses', '▁in', '▁a', '▁three', '-', 'month', '▁period', '.', '<0x0A>', 'If', '▁a', '▁claim', 'ant', '▁in', 'ad', 'vert', 'ently', '▁makes', '▁an', '▁error', '▁with', '▁an', '▁expense', '▁claim', '▁and', '▁has', '▁been', '▁re', 'imb', 'urs', 'ed', ',', '▁G', 'NE', 'I', '▁will', '▁recover', '▁the', '▁amount', '▁from', '▁them', '.', '<0x0A>', 'Exp', 'enses', '▁must', '▁not', '▁be', '▁used', '▁as', '▁a', '▁way', '▁of', '▁reward', 'ing', '▁people', '▁or', '▁encouraging', '▁them', '▁to', '▁work', '▁in', '▁remote', '▁locations', '.', '<0x0A>', 'G', 'NE', 'I', '▁will', '▁not', '▁meet', '▁the', '▁cost', '▁of', '▁expenses', '▁for', '▁sp', 'ouses', ',', '▁partners', ',', '▁other', '▁family', '▁members', '▁or', '▁friends', '▁of', '▁G', 'NE', 'I', '▁staff', '.', '<0x0A>', 'Claim', 's', '▁are', '▁checked', '▁by', '▁Account', 's', '▁Pay', 'able', '▁staff', '▁and', '▁any', '▁deemed', '▁to', '▁be', '▁fraud', 'ulent', '▁will', '▁be', '▁investigated', '▁and', '▁referred', '▁to', '▁G', 'NE', 'I', '’', 's', '▁Internal', '▁Aud', 'itors', '.', '▁If', '▁a', '▁claim', 'ant', '▁is', '▁found', '▁to', '▁have', '▁submitted', '▁a', '▁fraud', 'ulent', '▁claim', '▁they', '▁will', '▁face', '▁discipl', 'inary', '▁proceedings', ',', '▁leading', '▁to', '▁pen', 'alties', '▁up', '▁to', '▁and', '▁including', '▁summary', '▁dismiss', 'al', '.', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '4', '.', '▁Travel', '<0x0A>', 'This', '▁section', '▁details', '▁what', '▁can', '▁be', '▁claimed', '▁as', '▁travel', '▁expenses', '▁and', '▁provides', '▁information', '▁on', '▁specific', '▁modes', '▁of', '▁transport', '▁and', '▁other', '▁expenses', '▁associated', '▁with', '▁travel', '.', '<0x0A>', '<0x0A>', 'General', '▁guidance', '▁for', '▁travelling', '▁on', '▁G', 'NE', 'I', '▁business', '<0x0A>', 'The', '▁cost', '▁of', '▁business', '▁travel', ',', '▁that', '▁is', ',', '▁jour', 'neys', '▁away', '▁from', '▁your', '▁normal', '▁place', '▁of', '▁work', '▁while', '▁undert', 'aking', '▁G', 'NE', 'I', '▁business', '▁can', '▁be', '▁claimed', '.', '<0x0A>', 'In', '▁line', '▁with', '▁the', '▁G', 'NE', 'I', '▁travel', '▁policy', ',', '▁all', '▁business', '▁trips', '▁in', '▁the', '▁UK', '▁or', '▁abroad', '▁must', '▁be', '▁author', 'ised', '▁in', '▁writing', '▁in', '▁advance', '▁by', '▁the', '▁G', 'NE', 'I', '▁budget', '▁holder', '▁responsible', '▁for', '▁making', '▁the', '▁funds', '▁available', '.', '▁It', '▁is', '▁recommended', '▁that', '▁this', '▁is', '▁attached', '▁to', '▁the', '▁claim', '.', '<0x0A>', 'The', '▁budget', '▁holder', '▁/', '▁manager', '▁must', '▁agree', '▁that', '▁the', '▁travel', '▁is', '▁necessary', '▁and', '▁that', '▁alternative', '▁methods', '▁such', '▁as', '▁conference', '▁call', ',', '▁video', '▁conference', ',', '▁phone', '▁or', '▁email', '▁cannot', '▁be', '▁used', '▁or', '▁are', '▁not', '▁appropriate', '.', '▁Consider', 'ation', '▁should', '▁also', '▁be', '▁given', '▁as', '▁to', '▁how', '▁to', '▁minim', 'ise', '▁the', '▁environmental', '▁impact', '▁of', '▁travel', '.', '<0x0A>', 'This', '▁author', 'isation', '▁ensures', '▁that', '▁anticipated', '▁costs', '▁are', '▁in', '▁line', '▁with', '▁department', 'al', '▁plans', ',', '▁are', '▁eligible', '▁under', '▁the', '▁terms', '▁of', '▁any', '▁associated', '▁funding', ',', '▁and', '▁that', '▁subsequent', '▁expense', '▁claims', '▁can', '▁be', '▁submitted', '▁with', '▁confidence', ',', '▁subject', '▁to', '▁the', '▁provisions', '▁of', '▁this', '▁policy', '.', '<0x0A>', 'G', 'NE', 'I', '▁has', '▁preferred', '▁travel', '▁suppliers', '▁for', '▁air', 'line', ',', '▁hotel', '▁and', '▁car', '▁hire', '▁book', 'ings', '▁details', '▁on', '▁these', '▁suppliers', '▁can', '▁be', '▁found', '▁on', '▁the', '▁Pro', 'c', 'ure', 'ment', '▁website', '.', '▁These', '▁suppliers', '▁should', '▁be', '▁used', '▁for', '▁travel', '▁book', 'ings', '▁where', '▁possible', '.', '<0x0A>', '<0x0A>', 'Private', '▁travel', '▁costs', '<0x0A>', 'The', '▁following', '▁cannot', '▁be', '▁claimed', ':', '<0x0A>', 'Tr', 'avel', '▁between', '▁home', '▁and', '▁normal', '▁place', '▁of', '▁work', '.', '<0x0A>', 'Bus', 'iness', '▁travel', '▁broad', 'ly', '▁similar', '▁to', '▁the', '▁claim', 'ant', '’', 's', '▁normal', '▁comm', 'ute', '.', '<0x0A>', 'Rec', 're', 'ational', '▁travel', '▁and', '▁accommodation', '▁at', '▁or', '▁near', '▁the', '▁business', '▁travel', '▁destination', '.', '<0x0A>', 'Tr', 'avel', '▁or', '▁accommodation', '▁for', '▁family', '▁or', '▁friends', '▁accompanying', '▁the', '▁claim', 'ant', '▁on', '▁the', '▁business', '▁journey', '.', '<0x0A>', '<0x0A>', 'Rate', '▁of', '▁exchange', '<0x0A>', 'If', '▁a', '▁credit', '▁card', '▁was', '▁used', '▁to', '▁pay', '▁for', '▁overseas', '▁expenses', ',', '▁the', '▁rate', '▁charged', '▁by', '▁the', '▁credit', '▁card', '▁iss', 'uer', '▁can', '▁be', '▁claimed', '.', '▁A', '▁copy', '▁of', '▁the', '▁statement', '▁should', '▁be', '▁used', '▁to', '▁evidence', '▁the', '▁rate', '▁used']\n",
      "  Chosen Tokens - ['<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<s>', '▁Answer', ':', '▁{\"', 'Class', 'ification', '▁T', '1', '\":', '▁Policy', '▁Vi', 'ol', 'ated', ',', '<0x0A>', '▁▁▁', '▁\"', 'Re', 'im', 'bur', 'se', 'ment', '▁Am', 'ount', '\":', '▁£', '0', '.', '0', ',', '<0x0A>', '▁▁▁', '▁\"', 'Class', 's', 'if', 'c', 'ation', '▁T', '2', '\":', '▁Further', '▁Clar', 'ification', '▁Required', ',', '<0x0A>', '▁▁▁', '▁\"', 'Re', 'asons', '\":', '▁...', '}', '▁', '<0x0A>', '▁▁▁', '▁<', 'e', 'os', '>', '<0x0A>']\n",
      "  Rejected Tokens - ['<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<s>', '▁Answer', ':', '▁{\"', 'Class', 'ification', '▁T', '1', '\":', '▁Policy', '▁Vi', 'ol', 'ated', ',', '<0x0A>', '▁▁▁', '▁\"', 'Re', 'im', 'bur', 'se', 'ment', '▁Am', 'ount', '\":', '▁£', '2', '1', '7', '0', '.', '0', ',', '<0x0A>', '▁▁▁', '▁\"', 'Class', 's', 'if', 'c', 'ation', '▁T', '2', '\":', '▁Further', '▁Clar', 'ification', '▁Required', ',', '}', '▁', '<0x0A>', '▁▁▁', '▁<', 'e', 'os', '>', '<0x0A>']\n",
      "Sample 1:\n",
      "  Prompt Tokens - ['<s>', '▁A', '▁<<', '<', 'POL', 'IC', 'Y', '>>>', '▁and', '▁a', '▁<<', '<', 'SC', 'EN', 'AR', 'IO', '>>>', '▁will', '▁be', '▁provided', ',', '▁you', '▁are', '▁G', 'RE', 'AT', '▁at', '▁analys', 'ing', '▁the', '▁<<', '<', 'SC', 'EN', 'AR', 'IO', '>>>', '▁according', '▁to', '▁the', '▁<<', '<', 'POL', 'IC', 'Y', '>>', '>.', '▁Your', '▁answer', '▁M', 'UST', '▁and', '▁and', '▁CAN', '▁ON', 'LY', '▁be', '▁a', '▁valid', '▁json', '▁format', ',', '▁having', '▁', '4', '▁text', '▁fields', ':', '▁Class', 'ification', '▁T', '1', ',', '▁Re', 'im', 'bur', 'se', 'ment', '▁Am', 'ount', ',', '▁Class', 'ification', '▁T', '2', ',', '▁and', '▁Re', 'asons', '.', '▁Your', '▁answer', '▁should', '▁follow', '▁these', '▁structures', ':', '▁', '<0x0A>', '▁', '1', ')', '▁\"', 'Class', 'ification', '▁T', '1', '\"', '▁:', 'you', '▁M', 'UST', '▁choose', '▁the', '▁activity', '▁in', '▁the', '▁<<', '<', 'sc', 'enario', '>>>', '▁is', \"▁'\", 'Policy', '▁Vi', 'ol', 'ated', \"'\", '▁or', \"▁'\", 'Policy', '▁Not', '▁Vi', 'ol', 'ated', \"'\", '<0x0A>', '▁', '<0x0A>', '▁', '2', ')', '▁\"', 'Re', 'im', 'bur', 'se', 'ment', '▁Am', 'ount', '\"', '▁:', '▁Answer', '▁according', '▁to', '▁the', '▁Policy', '▁the', '▁amount', '▁of', '▁money', '▁can', '▁be', '▁re', 'imb', 'urs', 'ed', '▁totally', '▁', '<0x0A>', '▁', '3', ')', '▁\"', 'Class', 'ification', '▁T', '2', '\"', '▁:', '▁Comp', 'are', '▁to', '▁the', '▁requested', '▁re', 'im', 'bur', 'se', 'ment', '▁amount', '▁in', '▁the', '▁scenario', ',', '▁and', '▁answer', '▁if', '▁the', '▁claim', '▁is', \"▁'\", 'F', 'ully', '▁Re', 'imb', 'urs', 'able', \"'\", \"/'\", 'Part', 'ially', '▁Re', 'imb', 'urs', 'able', \"'\", \"/'\", 'Not', '▁Re', 'imb', 'urs', 'able', \"'\", \"/'\", 'F', 'ur', 'ther', '▁Clar', 'ification', '▁Required', \"'.\", '▁', '<0x0A>', '▁', '4', ')', '▁give', '▁reasons', '▁for', '▁your', '▁choices', '▁according', '▁to', '▁the', '▁<<', '<', 'POL', 'IC', 'Y', '>>', '>.', '<0x0A>', '▁▁▁', '▁<<', '<<', '<', 'Sc', 'enario', '▁Start', ':', '<0x0A>', '<0x0A>', 'For', '▁a', '▁series', '▁of', '▁workshops', '▁in', '▁Nigeria', ',', '▁I', '▁stayed', '▁at', '▁a', '▁hotel', '▁for', '▁', '8', '▁days', ',', '▁with', '▁the', '▁room', '▁cost', 'ing', '▁£', '1', '9', '5', '▁and', '▁meals', '▁(', 'break', 'fast', '▁and', '▁lunch', ')', '▁included', '.', '▁I', '▁also', '▁in', 'cur', 'red', '▁£', '2', '7', '▁on', '▁two', '▁external', '▁lun', 'ches', '▁and', '▁£', '2', '0', '▁on', '▁one', '▁external', '▁dinner', '▁due', '▁to', '▁private', '▁reasons', '.', '▁What', '▁amount', '▁can', '▁I', '▁get', '▁re', 'imb', 'urs', 'ed', '?', '<0x0A>', '<0x0A>', 'Sc', 'enario', '▁End', '>>', '>>', '><', '<<<<', 'Policy', '▁Start', ':', '<0x0A>', '<0x0A>', 'G', 'NE', 'I', '▁Exp', 'enses', '▁Policy', '<0x0A>', 'Summary', '▁Gu', 'id', 'ance', '<0x0A>', 'Subject', '<0x0A>', 'Page', '▁No', '.', '<0x0A>', 'Tr', 'ain', '▁Travel', '<0x0A>', 'This', '▁should', '▁be', '▁standard', '▁class', '▁unless', '▁a', '▁heavily', '▁discount', 'ed', '▁first', '-', 'class', '▁ticket', '▁is', '▁book', 'ed', '▁significantly', '▁in', '▁advance', '▁of', '▁the', '▁date', '▁of', '▁travel', '▁via', '▁G', 'NE', 'I', \"'\", 's', '▁approved', '▁travel', '▁provider', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'T', 'axis', '<0x0A>', 'T', 'axis', '▁may', '▁be', '▁used', '▁where', '▁there', '▁is', '▁a', '▁clear', '▁requirement', '▁or', '▁where', '▁they', '▁are', '▁the', '▁most', '▁economic', '▁practical', '▁means', '▁of', '▁transport', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Bus', 'iness', '▁M', 'ile', 'age', '<0x0A>', 'C', 'ars', '▁-', '▁', '5', '0', 'p', '▁per', '▁mile', '▁for', '▁the', '▁first', '▁', '1', '1', ',', '0', '0', '0', '▁miles', '▁in', '▁a', '▁tax', '▁year', '▁and', '▁', '2', '8', 'p', '▁per', '▁mile', '▁there', 'after', ',', '▁Motor', 'cy', 'cles', '▁-', '▁', '2', '6', 'p', '▁per', '▁mile', ',', '▁B', 'icy', 'cles', '▁-', '▁', '2', '2', 'p', '▁per', '▁mile', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Air', '▁Travel', '<0x0A>', 'This', '▁should', '▁be', '▁book', 'ed', '▁using', '▁G', 'NE', 'I', '’', 's', '▁preferred', '▁travel', '▁provider', '▁where', '▁possible', '.', '▁For', '▁flights', '▁of', '▁five', '▁hours', '▁or', '▁less', ',', '▁staff', '▁should', '▁book', '▁restricted', '▁tickets', '▁in', '▁economy', '.', '▁For', '▁flights', '▁lasting', '▁five', '▁hours', '▁or', '▁longer', '▁staff', '▁may', '▁travel', '▁premium', '▁economy', '.', '▁Where', '▁a', '▁long', '▁international', '▁flight', '▁is', '▁immediately', '▁followed', '▁by', '▁a', '▁presentation', '▁or', '▁meeting', ',', '▁staff', '▁may', '▁travel', '▁by', '▁business', '▁class', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Hot', 'el', '▁R', 'ates', '<0x0A>', 'UK', ':', '▁should', '▁not', '▁exceed', '▁£', '2', '1', '0', '▁including', '▁V', 'AT', '▁(', 'room', '▁only', ')', '▁in', '▁London', '▁or', '▁£', '1', '4', '0', '▁including', '▁V', 'AT', '▁(', 'room', '▁only', ')', '▁outside', '▁of', '▁London', '.', '▁The', '▁cost', '▁of', '▁accommodation', '▁at', '▁or', '▁near', '▁the', '▁normal', '▁place', '▁of', '▁work', '▁wouldn', '’', 't', '▁qualify', '▁as', '▁business', '▁travel', '▁and', '▁would', '▁therefore', '▁be', '▁subject', '▁to', '▁tax', '.', '<0x0A>', 'Over', 'se', 'as', ':', '▁not', '▁subject', '▁to', '▁a', '▁fixed', '▁rate', '▁or', '▁limit', '.', '▁Cl', 'aim', 'ants', '▁should', '▁instead', '▁look', '▁to', '▁apply', '▁this', '▁policy', \"'\", 's', '▁general', '▁principles', '▁(', 'section', '▁', '3', ')', '▁when', '▁assess', 'ing', '▁reason', 'ab', 'len', 'ess', '▁and', '▁use', '▁either', '▁H', 'M', 'RC', \"'\", 's', '▁overseas', '▁bench', 'mark', '▁rates', '▁and', '▁/', '▁or', '▁this', '▁policy', \"'\", 's', '▁equivalent', '▁UK', '▁rates', '▁for', '▁guidance', '<0x0A>', '<0x0A>', '<0x0A>', 'Me', 'als', '▁whilst', '▁away', '▁overnight', '<0x0A>', 'UK', '▁Break', 'fast', ':', '▁maximum', '▁of', '▁£', '1', '2', '▁rece', 'ipt', 'ed', '▁including', '▁V', 'AT', '▁and', '▁service', ';', '▁unre', 'ce', 'ipt', 'ed', '▁£', '5', '.', '5', '<0x0A>', 'UK', '▁L', 'unch', ':', '▁maximum', '▁of', '▁£', '1', '8', '▁rece', 'ipt', 'ed', '▁including', '▁V', 'AT', '▁and', '▁service', ';', '▁unre', 'ce', 'ipt', 'ed', '▁£', '6', '<0x0A>', 'UK', '▁D', 'inner', ':', '▁maximum', '▁of', '▁£', '3', '6', '▁rece', 'ipt', 'ed', '▁including', '▁V', 'AT', '▁and', '▁service', ',', '▁unre', 'ce', 'ipt', 'ed', '▁£', '1', '7', '.', '<0x0A>', 'UK', '▁', '2', '4', '-', 'hour', '▁rate', ':', '▁In', '▁line', '▁with', '▁the', '▁above', ',', '▁up', '▁to', '▁£', '6', '6', '▁rece', 'ipt', 'ed', '▁(', '£', '2', '8', '▁unre', 'ce', 'ipt', 'ed', ').', '<0x0A>', 'Over', 'se', 'as', ':', '▁not', '▁subject', '▁to', '▁a', '▁fixed', '▁rate', '▁or', '▁limit', '.', '▁Cl', 'aim', 'ants', '▁should', '▁instead', '▁look', '▁to', '▁apply', '▁this', '▁policy', \"'\", 's', '▁general', '▁principles', '▁(', 'section', '▁', '3', ')', '▁when', '▁assess', 'ing', '▁reason', 'ab', 'len', 'ess', '▁and', '▁use', '▁either', '▁H', 'M', 'RC', \"'\", 's', '▁overseas', '▁bench', 'mark', '▁rates', '▁and', '▁/', '▁or', '▁this', '▁policy', \"'\", 's', '▁equivalent', '▁UK', '▁rates', '▁for', '▁guidance', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Line', '▁R', 'ental', '<0x0A>', 'Line', '▁rental', '▁cannot', '▁be', '▁claimed', ';', '▁only', '▁the', '▁cost', '▁of', '▁business', '▁calls', '▁on', '▁a', '▁call', '-', 'by', '-', 'call', '▁basis', '▁can', '▁be', '▁claimed', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Broad', 'band', '<0x0A>', 'Home', '▁internet', '▁connection', '▁cannot', '▁be', '▁claimed', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'St', 'aff', '▁and', '▁Student', '▁Enter', 't', 'aining', '<0x0A>', 'This', '▁means', '▁food', '▁or', '▁drink', '▁for', '▁two', '▁or', '▁more', '▁members', '▁of', '▁staff', '▁or', '▁registered', '▁G', 'NE', 'I', '▁students', '▁in', '▁connection', '▁with', '▁G', 'NE', 'I', '▁business', '▁activities', '.', '▁The', '▁cost', '▁of', '▁entertaining', '▁should', '▁not', '▁exceed', '▁£', '2', '2', '▁per', '▁head', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'St', 'aff', '▁Social', '▁Fun', 'ctions', '<0x0A>', 'Events', '▁that', '▁are', '▁not', '▁in', '▁connection', '▁to', '▁G', 'NE', 'I', '▁business', '▁activities', '▁e', '.', 'g', '.', '▁Christmas', '▁lun', 'ches', '/', 'part', 'ies', ',', '▁end', '▁of', '▁term', '▁social', 's', ',', '▁retirement', '▁parties', '▁etc', '.', '▁The', '▁G', 'NE', 'I', '▁department', '▁contribution', '▁should', '▁be', '▁no', '▁more', '▁than', '▁£', '2', '2', '▁per', '▁head', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Bus', 'iness', '▁entertaining', '<0x0A>', 'Cost', '▁should', '▁be', '▁appropriate', '▁and', '▁not', '▁exceed', '▁£', '4', '4', '▁per', '▁head', '▁including', '▁alcohol', '▁and', '▁service', '▁unless', '▁exceptional', '▁circumstances', '▁apply', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'Pro', 'f', 'ess', 'ional', '▁Sub', 'scriptions', '<0x0A>', 'G', 'NE', 'I', '▁will', '▁only', '▁re', 'im', 'bur', 'se', '▁staff', ',', '▁or', '▁pay', '▁on', '▁their', '▁behalf', ',', '▁annual', '▁sub', 'scriptions', '▁or', '▁membership', 's', '▁to', '▁a', '▁professional', '▁body', '▁where', '▁either', ':', '<0x0A>', 'S', 'av', 'ings', '▁to', '▁G', 'NE', 'I', '▁ar', 'ising', '▁from', '▁membership', ',', '▁for', '▁example', '▁reduced', '▁conference', '▁attendance', '▁fees', ',', '▁exceed', '▁the', '▁cost', '▁of', '▁membership', ',', '▁or', '<0x0A>', 'Members', 'hip', '▁is', '▁mandatory', '▁in', '▁order', '▁to', '▁be', '▁able', '▁to', '▁teach', '▁on', '▁a', '▁profession', 'ally', '▁acc', 'red', 'ited', '▁course', '.', '<0x0A>', 'In', '▁all', '▁cases', ',', '▁the', '▁professional', '▁body', '▁must', '▁feature', '▁in', '▁the', '▁list', '▁of', '▁approved', '▁organisations', '▁published', '▁by', '▁H', 'M', 'RC', '.', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '1', '.', '▁Introduction', '▁', '<0x0A>', 'This', '▁Exp', 'enses', '▁Policy', '▁(', '‘', 'the', '▁Policy', '’', ')', '▁provides', '▁guidance', '▁to', '▁all', '▁individuals', '▁claiming', '▁re', 'im', 'bur', 'se', 'ment', '▁of', '▁reasonable', '▁expenses', '▁in', 'cur', 'red', '▁in', '▁connection', '▁with', '▁G', 'NE', 'I', '▁business', '.', '<0x0A>', 'G', 'NE', 'I', '▁is', '▁a', '▁charity', '▁and', '▁a', '▁large', '▁recipient', '▁of', '▁public', '▁funds', '▁from', '▁UK', 'RI', '▁and', '▁the', '▁Office', '▁for', '▁Students', ',', '▁grants', '▁from', '▁other', '▁public', '▁bodies', ',', '▁char', 'ities', '▁and', '▁fees', '▁paid', '▁by', '▁students', '.', '▁All', '▁expend', 'iture', '▁should', '▁be', '▁appropriate', '▁and', '▁modest', '▁in', '▁scale', '.', '<0x0A>', 'This', '▁policy', '▁has', '▁been', '▁prepared', '▁in', '▁accordance', '▁with', '▁In', 'come', '▁Tax', '▁and', '▁National', '▁Insurance', '▁Contribut', 'ion', '▁regulations', '▁and', '▁Her', '▁Maj', 'esty', '’', 's', '▁Rev', 'enue', '▁and', '▁Custom', 's', '▁(', 'H', 'M', 'RC', ')', '▁regulations', '.', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '2', '.', '▁S', 'cope', '<0x0A>', 'This', '▁policy', '▁applies', '▁to', '▁all', '▁spend', '▁on', '▁G', 'NE', 'I', '▁activities', '▁including', '▁from', '▁research', '▁grants', '▁and', '▁discret', 'ion', 'ary', '▁accounts', '▁as', '▁well', '▁as', '▁department', 'al', '▁codes', '.', '▁', '<0x0A>', 'This', '▁policy', '▁covers', '▁the', '▁following', '▁areas', '▁of', '▁expend', 'iture', ':', '<0x0A>', 'Tr', 'avel', '<0x0A>', 'Over', 'night', '▁costs', '▁and', '▁allow', 'ances', '<0x0A>', 'Tele', 'phone', '▁and', '▁internet', '▁costs', '<0x0A>', 'Enter', 'tainment', '▁&', '▁Hospital', 'ity', '<0x0A>', 'Tr', 'aining', '<0x0A>', 'Sub', 'scriptions', '<0x0A>', 'Adv', 'ances', '<0x0A>', 'Other', '▁Ex', 'pend', 'iture', '<0x0A>', 'For', '▁items', '▁that', '▁do', '▁not', '▁fall', '▁under', '▁any', '▁of', '▁the', '▁above', '▁head', 'ings', ',', '▁please', '▁refer', '▁to', '▁your', '▁local', '▁finance', '▁team', '▁or', '▁Account', 's', '▁Pay', 'able', '▁at', '▁expenses', '@', 'g', 'ne', 'i', '.', 'ac', '.', 'uk', '.', '<0x0A>', 'For', '▁expense', '▁claim', '▁procedures', ',', '▁please', '▁refer', '▁to', '▁Anne', 'x', '▁', '1', '.', '<0x0A>', 'Gener', 'ally', ',', '▁G', 'NE', 'I', '▁will', '▁not', '▁re', 'im', 'bur', 'se', '▁individuals', '▁for', ':', '<0x0A>', 'Broad', 'band', '/', 'Intern', 'et', '▁Prov', 'ision', '<0x0A>', 'Equ', 'ip', 'ment', '<0x0A>', 'F', 'ines', '<0x0A>', 'G', 'ifts', '<0x0A>', 'Ins', 'urance', '<0x0A>', 'Mobile', '▁Phone', '▁Contract', 's', '/', 'Hard', 'ware', '<0x0A>', 'Person', 'al', '▁Ex', 'pend', 'iture', '<0x0A>', 'Pro', 'f', 'ess', 'ional', '▁Sub', 'scriptions', '<0x0A>', 'Station', 'ery', '<0x0A>', 'For', '▁further', '▁details', '▁on', '▁the', '▁above', '▁excl', 'usions', ',', '▁please', '▁refer', '▁to', '▁Anne', 'x', '▁', '2', '.', '<0x0A>', 'Information', '▁on', '▁V', 'AT', '▁aspects', '▁which', '▁are', '▁common', '▁to', '▁all', '▁claims', '▁can', '▁be', '▁found', '▁at', '▁Anne', 'x', '▁', '3', '.', '<0x0A>', 'This', '▁policy', '▁also', '▁provides', '▁information', '▁on', '▁subs', 'istence', '▁payments', '▁for', '▁visiting', '▁Research', '▁F', 'ell', 'ows', ',', '▁please', '▁refer', '▁to', '▁Anne', 'x', '▁', '4', '.', '<0x0A>', 'Child', 'care', '▁and', '▁caring', '▁responsibility', '▁can', '▁be', '▁claimed', '▁in', '▁certain', '▁circumstances', ',', '▁please', '▁refer', '▁to', '▁Anne', 'x', '▁', '5', '.', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '3', '.', '▁General', '▁Prin', 'ciples', '<0x0A>', 'G', 'NE', 'I', '▁employees', '▁and', '▁all', '▁others', '▁engaged', '▁in', '▁G', 'NE', 'I', '▁activity', '▁(', 'collect', 'ively', '▁claim', 'ants', ')', '▁will', '▁be', '▁re', 'imb', 'urs', 'ed', '▁for', '▁the', '▁actual', '▁cost', '▁of', '▁expenses', '▁in', 'cur', 'red', ',', '▁wh', 'olly', ',', '▁exclusively', '▁and', '▁necessarily', '▁in', '▁the', '▁performance', '▁of', '▁G', 'NE', 'I', '▁activity', '▁as', '▁presc', 'ribed', '▁in', '▁this', '▁policy', '.', '<0x0A>', 'The', '▁expense', '▁must', '▁be', '▁just', 'ifiable', '▁and', '▁reasonable', '▁according', '▁to', '▁the', '▁information', '▁in', '▁this', '▁policy', ',', '▁and', '▁the', '▁claim', '▁should', '▁always', '▁be', '▁prepared', '▁honestly', ',', '▁legally', '▁and', '▁respons', 'ibly', '.', '▁Any', '▁bre', 'ach', '▁of', '▁the', '▁policy', '▁could', '▁lead', '▁to', '▁discipl', 'inary', '▁action', ',', '▁up', '▁to', '▁and', '▁including', '▁dismiss', 'al', '.', '<0x0A>', 'Wh', 'ilst', '▁this', '▁policy', '▁aims', '▁to', '▁provide', '▁comprehensive', '▁guidance', '▁on', '▁re', 'imb', 'urs', 'able', '▁out', '-', 'of', '-', '▁pocket', '▁expend', 'iture', ',', '▁it', '▁is', '▁recommended', '▁that', '▁staff', '▁still', '▁seek', '▁advance', '▁approval', '▁from', '▁their', '▁budget', '▁holder', '▁or', '▁department', '▁manager', '▁in', '▁any', '▁situation', '▁where', '▁interpretation', '▁of', '▁this', '▁policy', '▁is', '▁in', '▁doubt', '.', '<0x0A>', 'Em', 'ploy', 'ees', '▁should', '▁be', '▁able', '▁to', '▁proc', 'ure', '▁the', '▁majority', '▁of', '▁goods', '▁and', '▁services', '▁required', '▁for', '▁G', 'NE', 'I', '▁business', '▁using', '▁the', '▁standard', '▁purchase', '▁to', '▁payment', '▁procedures', '▁using', '▁the', '▁G', 'NE', 'IF', 'in', 'ance', '▁system', '.', '<0x0A>', 'It', '▁is', '▁the', '▁responsibility', '▁of', '▁author', 'ised', '▁sign', 'ator', 'ies', '▁to', '▁ensure', '▁that', '▁this', '▁expenses', '▁policy', '▁is', '▁up', 'held', ';', '▁and', '▁it', '▁should', '▁be', '▁noted', '▁that', '▁individual', '▁claims', '▁falling', '▁outside', '▁of', '▁the', '▁policy', '▁may', '▁be', '▁subject', '▁to', '▁review', '▁and', '/', 'or', '▁re', 'jection', '▁by', '▁Finance', '.', '<0x0A>', 'E', 'lect', 'ronic', '▁images', '▁of', '▁original', '▁rece', 'i', 'pts', '▁must', '▁accompany', '▁all', '▁claims', '.', '▁Credit', '▁card', '▁sl', 'ips', '▁or', '▁credit', '▁card', '/', 'bank', '▁statements', '▁will', '▁not', '▁be', '▁accepted', '▁as', '▁evidence', '▁of', '▁business', '▁expend', 'iture', '.', '▁All', '▁rece', 'i', 'pts', '▁used', '▁should', '▁include', '▁details', '▁of', '▁what', '▁goods', '▁or', '▁services', '▁have', '▁been', '▁purchased', '.', '▁Original', '▁paper', '▁rece', 'i', 'pts', '▁do', '▁not', '▁need', '▁to', '▁be', '▁stored', '▁locally', ',', '▁unless', '▁they', '▁relate', '▁to', '▁sponsored', '▁research', '▁and', '▁it', '▁is', '▁explicit', '▁in', '▁the', '▁terms', '▁of', '▁the', '▁grant', '▁that', '▁original', '▁paper', '▁rece', 'i', 'pts', '▁need', '▁to', '▁be', '▁retained', '.', '<0x0A>', 'When', '▁making', '▁a', '▁claim', '▁for', '▁a', '▁group', ',', '▁the', '▁most', '▁senior', '▁person', '▁present', '▁should', '▁pay', '▁for', '▁the', '▁expend', 'iture', '▁and', '▁make', '▁the', '▁claim', '.', '<0x0A>', 'Exp', 'enses', '▁should', '▁be', '▁submitted', '▁as', '▁soon', '▁as', '▁possible', '▁after', '▁they', '▁have', '▁been', '▁in', 'cur', 'red', '.', '▁Cl', 'aim', 's', '▁must', '▁be', '▁made', '▁within', '▁three', '▁months', '.', '▁Appro', 'val', '▁should', '▁be', '▁gained', '▁from', '▁the', '▁relevant', '▁School', '▁or', '▁PS', '▁Head', '▁of', '▁Finance', '▁if', '▁the', '▁claim', '▁is', '▁over', '▁three', '▁months', '▁old', '.', '<0x0A>', 'It', '▁costs', '▁a', '▁fixed', '▁amount', '▁to', '▁process', '▁any', '▁claim', ',', '▁so', '▁claim', 'ants', '▁should', '▁ende', 'av', 'our', '▁not', '▁to', '▁submit', '▁claims', '▁for', '▁less', '▁than', '▁£', '3', '3', ',', '▁unless', '▁they', '▁represent', '▁the', '▁total', '▁of', '▁expenses', '▁in', '▁a', '▁three', '-', 'month', '▁period', '.', '<0x0A>', 'If', '▁a', '▁claim', 'ant', '▁in', 'ad', 'vert', 'ently', '▁makes', '▁an', '▁error', '▁with', '▁an', '▁expense', '▁claim', '▁and', '▁has', '▁been', '▁re', 'imb', 'urs', 'ed', ',', '▁G', 'NE', 'I', '▁will', '▁recover', '▁the', '▁amount', '▁from', '▁them', '.', '<0x0A>', 'Exp', 'enses', '▁must', '▁not', '▁be', '▁used', '▁as', '▁a', '▁way', '▁of', '▁reward', 'ing', '▁people', '▁or', '▁encouraging', '▁them', '▁to', '▁work', '▁in', '▁remote', '▁locations', '.', '<0x0A>', 'G', 'NE', 'I', '▁will', '▁not', '▁meet', '▁the', '▁cost', '▁of', '▁expenses', '▁for', '▁sp', 'ouses', ',', '▁partners', ',', '▁other', '▁family', '▁members', '▁or', '▁friends', '▁of', '▁G', 'NE', 'I', '▁staff', '.', '<0x0A>', 'Claim', 's', '▁are', '▁checked', '▁by', '▁Account', 's', '▁Pay', 'able', '▁staff', '▁and', '▁any', '▁deemed', '▁to', '▁be', '▁fraud', 'ulent', '▁will', '▁be', '▁investigated', '▁and', '▁referred', '▁to', '▁G', 'NE', 'I', '’', 's', '▁Internal', '▁Aud', 'itors', '.', '▁If', '▁a', '▁claim', 'ant', '▁is', '▁found', '▁to', '▁have', '▁submitted', '▁a', '▁fraud', 'ulent', '▁claim', '▁they', '▁will', '▁face', '▁discipl', 'inary', '▁proceedings', ',', '▁leading', '▁to', '▁pen', 'alties', '▁up', '▁to', '▁and', '▁including', '▁summary', '▁dismiss', 'al', '.', '<0x0A>', '<0x0A>', '<0x0A>', '<0x0A>', '4', '.', '▁Travel', '<0x0A>', 'This', '▁section', '▁details', '▁what', '▁can', '▁be', '▁claimed', '▁as', '▁travel', '▁expenses', '▁and', '▁provides', '▁information', '▁on', '▁specific', '▁modes', '▁of', '▁transport', '▁and', '▁other', '▁expenses', '▁associated', '▁with', '▁travel', '.', '<0x0A>', '<0x0A>', 'General', '▁guidance', '▁for', '▁travelling', '▁on', '▁G', 'NE', 'I', '▁business', '<0x0A>', 'The', '▁cost', '▁of', '▁business', '▁travel', ',', '▁that', '▁is', ',', '▁jour', 'neys', '▁away', '▁from', '▁your', '▁normal', '▁place', '▁of', '▁work', '▁while', '▁undert', 'aking', '▁G', 'NE', 'I', '▁business', '▁can', '▁be', '▁claimed', '.', '<0x0A>', 'In', '▁line', '▁with', '▁the', '▁G', 'NE', 'I', '▁travel', '▁policy', ',', '▁all', '▁business', '▁trips', '▁in', '▁the', '▁UK', '▁or', '▁abroad', '▁must', '▁be', '▁author', 'ised', '▁in', '▁writing', '▁in', '▁advance', '▁by', '▁the', '▁G', 'NE', 'I', '▁budget', '▁holder', '▁responsible', '▁for', '▁making', '▁the', '▁funds', '▁available', '.', '▁It', '▁is', '▁recommended', '▁that', '▁this', '▁is', '▁attached', '▁to', '▁the', '▁claim', '.', '<0x0A>', 'The', '▁budget', '▁holder', '▁/', '▁manager', '▁must', '▁agree', '▁that', '▁the', '▁travel', '▁is', '▁necessary', '▁and', '▁that', '▁alternative', '▁methods', '▁such', '▁as', '▁conference', '▁call', ',', '▁video', '▁conference', ',', '▁phone', '▁or', '▁email', '▁cannot', '▁be', '▁used', '▁or', '▁are', '▁not', '▁appropriate', '.', '▁Consider', 'ation', '▁should', '▁also', '▁be', '▁given', '▁as', '▁to', '▁how', '▁to', '▁minim', 'ise', '▁the', '▁environmental', '▁impact', '▁of', '▁travel', '.', '<0x0A>', 'This', '▁author', 'isation', '▁ensures', '▁that', '▁anticipated', '▁costs', '▁are', '▁in', '▁line', '▁with', '▁department', 'al', '▁plans', ',', '▁are', '▁eligible', '▁under', '▁the', '▁terms', '▁of', '▁any', '▁associated', '▁funding', ',', '▁and', '▁that', '▁subsequent', '▁expense', '▁claims', '▁can', '▁be', '▁submitted', '▁with', '▁confidence', ',', '▁subject', '▁to', '▁the', '▁provisions', '▁of', '▁this', '▁policy', '.', '<0x0A>', 'G', 'NE', 'I', '▁has', '▁preferred', '▁travel', '▁suppliers', '▁for', '▁air', 'line', ',', '▁hotel', '▁and', '▁car', '▁hire', '▁book', 'ings', '▁details', '▁on', '▁these', '▁suppliers', '▁can', '▁be', '▁found', '▁on', '▁the', '▁Pro', 'c', 'ure', 'ment', '▁website', '.', '▁These', '▁suppliers', '▁should', '▁be', '▁used', '▁for', '▁travel', '▁book', 'ings', '▁where', '▁possible', '.', '<0x0A>', '<0x0A>', 'Private', '▁travel', '▁costs', '<0x0A>', 'The', '▁following', '▁cannot', '▁be', '▁claimed', ':', '<0x0A>', 'Tr', 'avel', '▁between', '▁home', '▁and', '▁normal', '▁place', '▁of', '▁work', '.', '<0x0A>', 'Bus', 'iness', '▁travel', '▁broad', 'ly', '▁similar', '▁to', '▁the', '▁claim', 'ant', '’', 's', '▁normal', '▁comm', 'ute', '.', '<0x0A>', 'Rec', 're', 'ational', '▁travel', '▁and', '▁accommodation', '▁at', '▁or', '▁near', '▁the', '▁business', '▁travel', '▁destination', '.', '<0x0A>', 'Tr', 'avel', '▁or', '▁accommodation', '▁for', '▁family', '▁or', '▁friends', '▁accompanying', '▁the', '▁claim', 'ant', '▁on', '▁the', '▁business', '▁journey', '.', '<0x0A>', '<0x0A>', 'Rate', '▁of', '▁exchange', '<0x0A>']\n",
      "  Chosen Tokens - ['<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<s>', '▁Answer', ':', '▁{\"', 'Class', 'ification', '▁T', '1', '\":', '▁Policy', '▁Vi', 'ol', 'ated', ',', '<0x0A>', '▁▁▁', '▁\"', 'Re', 'im', 'bur', 'se', 'ment', '▁Am', 'ount', '\":', '▁£', '1', '9', '5', '.', '0', ',', '<0x0A>', '▁▁▁', '▁\"', 'Class', 's', 'if', 'c', 'ation', '▁T', '2', '\":', '▁Part', 'ially', '▁Re', 'imb', 'urs', 'able', ',', '<0x0A>', '▁▁▁', '▁\"', 'Re', 'asons', '\":', '▁...', '}', '▁', '<0x0A>', '▁▁▁', '▁<', 'e', 'os', '>', '<0x0A>']\n",
      "  Rejected Tokens - ['<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<|im_end|>', '<s>', '▁Answer', ':', '▁{\"', 'Class', 'ification', '▁T', '1', '\":', '▁Policy', '▁Not', '▁Vi', 'ol', 'ated', ',', '<0x0A>', '▁▁▁', '▁\"', 'Re', 'im', 'bur', 'se', 'ment', '▁Am', 'ount', '\":', '▁£', '1', '9', '5', '.', '0', ',', '<0x0A>', '▁▁▁', '▁\"', 'Class', 's', 'if', 'c', 'ation', '▁T', '2', '\":', '▁F', 'ully', '▁Re', 'imb', 'urs', 'able', ',', '}', '▁', '<0x0A>', '▁▁▁', '▁<', 'e', 'os', '>', '<0x0A>']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "max_prompt_length = 2560  # Example value\n",
    "data_collator = CustomDataCollator(tokenizer, max_prompt_length)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset['test'], batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Get a single batch\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# Inspect tokens in the batch\n",
    "for i in range(len(batch['input_ids_prompt'])):\n",
    "    prompt_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids_prompt'][i])\n",
    "    chosen_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids_chosen'][i])\n",
    "    rejected_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids_rejected'][i])\n",
    "    \n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Prompt Tokens - {prompt_tokens}\")\n",
    "    print(f\"  Chosen Tokens - {chosen_tokens}\")\n",
    "    print(f\"  Rejected Tokens - {rejected_tokens}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceehuf/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 46.36 examples/s]\n",
      "/home/uceehuf/.local/lib/python3.11/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=2e-5,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=3072,\n",
    "    max_prompt_length=2560,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=5,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps=0.05,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results/\",\n",
    ")\n",
    "\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['prompt', 'chosen', 'rejected', 'input_ids_prompt', 'input_ids_chosen', 'input_ids_rejected']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mdata_collator)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get a single batch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Inspect tokens in the batch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_prompt\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m, in \u001b[0;36mCustomDataCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     22\u001b[0m     feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m rejected_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Use the parent class's collate method\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3245\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   3244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 3245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3248\u001b[0m     )\n\u001b[1;32m   3250\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['prompt', 'chosen', 'rejected', 'input_ids_prompt', 'input_ids_chosen', 'input_ids_rejected']"
     ]
    }
   ],
   "source": [
    "# Manually process a single batch\n",
    "\n",
    "max_prompt_length = 2560  # Example value\n",
    "data_collator = CustomDataCollator(tokenizer, max_prompt_length)\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset['test'], batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Get a single batch\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# Inspect tokens in the batch\n",
    "for i in range(len(batch['input_ids_prompt'])):\n",
    "    prompt_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids_prompt'][i])\n",
    "    chosen_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids_chosen'][i])\n",
    "    rejected_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids_rejected'][i])\n",
    "    \n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Prompt Tokens - {prompt_tokens}\")\n",
    "    print(f\"  Chosen Tokens - {chosen_tokens}\")\n",
    "    print(f\"  Rejected Tokens - {rejected_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunyu-zhu-23\u001b[0m (\u001b[33mucl_student\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/uceehuf/msc_const_ai-2/notebooks/wandb/run-20240702_161605-1xpfwluj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ucl_student/huggingface/runs/1xpfwluj' target=\"_blank\">icy-pyramid-34</a></strong> to <a href='https://wandb.ai/ucl_student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ucl_student/huggingface' target=\"_blank\">https://wandb.ai/ucl_student/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ucl_student/huggingface/runs/1xpfwluj' target=\"_blank\">https://wandb.ai/ucl_student/huggingface/runs/1xpfwluj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 118.56 MiB is free. Process 1870000 has 414.00 MiB memory in use. Process 1877952 has 414.00 MiB memory in use. Process 1881595 has 414.00 MiB memory in use. Including non-PyTorch memory, this process has 77.77 GiB memory in use. Of the allocated memory 72.36 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43morpo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:786\u001b[0m, in \u001b[0;36mORPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    783\u001b[0m compute_loss_context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_has_been_casted_to_bf16 \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compute_loss_context_manager():\n\u001b[0;32m--> 786\u001b[0m     loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_loss_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# force log the metrics\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_metrics(metrics, train_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:746\u001b[0m, in \u001b[0;36mORPOTrainer.get_batch_loss_metrics\u001b[0;34m(self, model, batch, train_eval)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the ORPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\u001b[39;00m\n\u001b[1;32m    738\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    740\u001b[0m (\n\u001b[1;32m    741\u001b[0m     policy_chosen_logps,\n\u001b[1;32m    742\u001b[0m     policy_rejected_logps,\n\u001b[1;32m    743\u001b[0m     policy_chosen_logits,\n\u001b[1;32m    744\u001b[0m     policy_rejected_logits,\n\u001b[1;32m    745\u001b[0m     policy_nll_loss,\n\u001b[0;32m--> 746\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenated_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m losses, chosen_rewards, rejected_rewards, log_odds_ratio, log_odds_chosen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39modds_ratio_loss(\n\u001b[1;32m    749\u001b[0m     policy_chosen_logps, policy_rejected_logps\n\u001b[1;32m    750\u001b[0m )\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# full ORPO loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:686\u001b[0m, in \u001b[0;36mORPOTrainer.concatenated_forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m    676\u001b[0m len_chosen \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchosen_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    678\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    679\u001b[0m     {\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shift_right(concatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    684\u001b[0m )\n\u001b[0;32m--> 686\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m all_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy_loss\u001b[39m(logits, labels):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/peft/peft_model.py:1430\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1429\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1033\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1034\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         use_cache,\n\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:757\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:688\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    685\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    686\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 688\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\u001b[39;49;00m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    699\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 118.56 MiB is free. Process 1870000 has 414.00 MiB memory in use. Process 1877952 has 414.00 MiB memory in use. Process 1881595 has 414.00 MiB memory in use. Including non-PyTorch memory, this process has 77.77 GiB memory in use. Of the allocated memory 72.36 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/uceehuf/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 255, in _read_packet_bytes\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/uceehuf/.local/lib/python3.11/site-packages/wandb/sdk/interface/router_sock.py\", line 27, in _read_message\n",
      "  File \"/home/uceehuf/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 285, in read_server_response\n",
      "  File \"/home/uceehuf/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py\", line 259, in _read_packet_bytes\n",
      "wandb.sdk.lib.sock_client.SockClientClosedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/uceehuf/.local/lib/python3.11/site-packages/wandb/sdk/interface/router.py\", line 70, in message_loop\n",
      "  File \"/home/uceehuf/.local/lib/python3.11/site-packages/wandb/sdk/interface/router_sock.py\", line 29, in _read_message\n",
      "wandb.sdk.interface.router.MessageRouterClosedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib64/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 93] Protocol not supported\n",
      "Call stack:\n",
      "  File \"/usr/lib64/python3.11/threading.py\", line 1002, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib64/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/uceehuf/.local/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "  File \"/usr/lib64/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/uceehuf/.local/lib/python3.11/site-packages/wandb/sdk/interface/router.py\", line 77, in message_loop\n",
      "Message: 'message_loop has been closed'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "orpo_trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
